{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"install/","text":"Prerequisites Provide access to Kubernetes cluster. You can also run it locally within a minikube . Make sure to provision enough memory (8G+) and CPU (8+) resources: All MacOS minikube start --memory = 8196 --cpus 8 minikube start --driver = hyperkit --memory = 8196 --cpus 8 Install Streaming Runtime Operator This oneliner installs the Streaming Runtime Operator in your Kubernetes environment: kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-operator/install.yaml' -n streaming-runtime It installs the streaming-runtime operator along with the custom resource definitions ( CRDs ) (such as ClusterStream , Stream and Processor ) and required roles and binding configurations. (Optional) Install Service Binding Operator If the Service Binding specification is used to manage the sensitive information in the streaming pipelines, you need to pre-install a compliant operator such as the VMWare-Tanzu Service Binding Operator : kubectl apply -f https://github.com/vmware-tanzu/servicebinding/releases/download/v0.7.1/service-bindings-0.7.1.yaml (Optional) Install RabbitMQ Cluster and Message Topology Operators If you decided to use the RabbitMQ auto-provisioning, based on RabbitMQ Cluster & Message Topology Operators the following managers and operators are required: kubectl apply -f \"https://github.com/rabbitmq/cluster-operator/releases/latest/download/cluster-operator.yml\" kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.3.1/cert-manager.yaml kubectl apply -f https://github.com/rabbitmq/messaging-topology-operator/releases/latest/download/messaging-topology-operator-with-certmanager.yaml Next Steps Follow the Overview for general understanding how the Streaming Runtime works or the Samples for various executable examples.","title":"Install"},{"location":"install/#prerequisites","text":"Provide access to Kubernetes cluster. You can also run it locally within a minikube . Make sure to provision enough memory (8G+) and CPU (8+) resources: All MacOS minikube start --memory = 8196 --cpus 8 minikube start --driver = hyperkit --memory = 8196 --cpus 8","title":"Prerequisites"},{"location":"install/#install-streaming-runtime-operator","text":"This oneliner installs the Streaming Runtime Operator in your Kubernetes environment: kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-operator/install.yaml' -n streaming-runtime It installs the streaming-runtime operator along with the custom resource definitions ( CRDs ) (such as ClusterStream , Stream and Processor ) and required roles and binding configurations.","title":"Install Streaming Runtime Operator"},{"location":"install/#optional-install-service-binding-operator","text":"If the Service Binding specification is used to manage the sensitive information in the streaming pipelines, you need to pre-install a compliant operator such as the VMWare-Tanzu Service Binding Operator : kubectl apply -f https://github.com/vmware-tanzu/servicebinding/releases/download/v0.7.1/service-bindings-0.7.1.yaml","title":"(Optional) Install Service Binding Operator"},{"location":"install/#optional-install-rabbitmq-cluster-and-message-topology-operators","text":"If you decided to use the RabbitMQ auto-provisioning, based on RabbitMQ Cluster & Message Topology Operators the following managers and operators are required: kubectl apply -f \"https://github.com/rabbitmq/cluster-operator/releases/latest/download/cluster-operator.yml\" kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.3.1/cert-manager.yaml kubectl apply -f https://github.com/rabbitmq/messaging-topology-operator/releases/latest/download/messaging-topology-operator-with-certmanager.yaml","title":"(Optional) Install RabbitMQ Cluster and Message Topology Operators"},{"location":"install/#next-steps","text":"Follow the Overview for general understanding how the Streaming Runtime works or the Samples for various executable examples.","title":"Next Steps"},{"location":"sr-technical-stack/","text":"Streaming-Lite For applications which rely on time-ordered delivery of events for a single key, but which do not perform any cross-key operations like joins or repartitioning. These applications include windowing in some cases but not others. Typically, streaming-lite applications run as pipelined workloads in an existing scheduler framework such as Kubernetes. Examples include ETL applications and time-series analysis. The Streaming Runtime SRP and SCS processor types are provided to support the Streaming Light use case. The built-in, general purpose SRP processor is capable of data partitioning, scaling, time-window aggregation and polyglot UDF functions. The SCS processor provide drop-in support for the large Spring Cloud Stream / Spring Cloud Function ecosystems. Streaming-Expert For applications which perform cross-stream analysis using joins and/or repartitioning, or applications which may perform dynamic stream operations (for example, windowing based on data conditions rather than time or rows). These applications typically using a pool of worker nodes, where the code and data is distributed to nodes using a framework-specific technique. Examples include Apache Spark, Apache Beam, and Apache Flink. Currently the Streaming Runtime offers a FSQL processor type that can run fully-fledged Apache Flink Streaming SQL queries. (in embedded mode only). Implementation Stack The Streaming Runtime implementation stack looks like this: TODO: Provide CRC documentation for the implementation stack.","title":"SR Stack"},{"location":"sr-technical-stack/#streaming-lite","text":"For applications which rely on time-ordered delivery of events for a single key, but which do not perform any cross-key operations like joins or repartitioning. These applications include windowing in some cases but not others. Typically, streaming-lite applications run as pipelined workloads in an existing scheduler framework such as Kubernetes. Examples include ETL applications and time-series analysis. The Streaming Runtime SRP and SCS processor types are provided to support the Streaming Light use case. The built-in, general purpose SRP processor is capable of data partitioning, scaling, time-window aggregation and polyglot UDF functions. The SCS processor provide drop-in support for the large Spring Cloud Stream / Spring Cloud Function ecosystems.","title":"Streaming-Lite"},{"location":"sr-technical-stack/#streaming-expert","text":"For applications which perform cross-stream analysis using joins and/or repartitioning, or applications which may perform dynamic stream operations (for example, windowing based on data conditions rather than time or rows). These applications typically using a pool of worker nodes, where the code and data is distributed to nodes using a framework-specific technique. Examples include Apache Spark, Apache Beam, and Apache Flink. Currently the Streaming Runtime offers a FSQL processor type that can run fully-fledged Apache Flink Streaming SQL queries. (in embedded mode only).","title":"Streaming-Expert"},{"location":"sr-technical-stack/#implementation-stack","text":"The Streaming Runtime implementation stack looks like this: TODO: Provide CRC documentation for the implementation stack.","title":"Implementation Stack"},{"location":"streaming-runtime-build/","text":"Build & Run Streaming Runtime Operator build instructions to build the operator, create a container image and upload it to container registry. CRDs Every time the CRDs under the ./crds folder are modified make sure to runt the regnerate the models and installation. Generate CRDs Java api and models ./scripts/generate-streaming-runtime-crd.sh Generated code is under the ./streaming-runtime/src/generated/java/com/vmware/tanzu/streaming folder Build operator installation yaml ./scripts/build-streaming-runtime-operator-installer.sh producing the install.yaml . The ./scripts/all.sh combines above two steps. Build the operator code and image ./mvnw clean install -Dnative -DskipTests spring-boot:build-image docker push ghcr.io/vmware-tanzu/streaming-runtimes/streaming-runtime:0.0.4-SNAPSHOT (For no-native build remove the -Dnative ). User Defined Functions Follow the User Defined Function documentation to learn how to implement and build UDFs , and how to use them from within a Processor resource.","title":"Build"},{"location":"streaming-runtime-build/#build-run","text":"","title":"Build &amp; Run"},{"location":"streaming-runtime-build/#streaming-runtime-operator","text":"build instructions to build the operator, create a container image and upload it to container registry.","title":"Streaming Runtime Operator"},{"location":"streaming-runtime-build/#crds","text":"Every time the CRDs under the ./crds folder are modified make sure to runt the regnerate the models and installation. Generate CRDs Java api and models ./scripts/generate-streaming-runtime-crd.sh Generated code is under the ./streaming-runtime/src/generated/java/com/vmware/tanzu/streaming folder Build operator installation yaml ./scripts/build-streaming-runtime-operator-installer.sh producing the install.yaml . The ./scripts/all.sh combines above two steps.","title":"CRDs"},{"location":"streaming-runtime-build/#build-the-operator-code-and-image","text":"./mvnw clean install -Dnative -DskipTests spring-boot:build-image docker push ghcr.io/vmware-tanzu/streaming-runtimes/streaming-runtime:0.0.4-SNAPSHOT (For no-native build remove the -Dnative ).","title":"Build the operator code and image"},{"location":"streaming-runtime-build/#user-defined-functions","text":"Follow the User Defined Function documentation to learn how to implement and build UDFs , and how to use them from within a Processor resource.","title":"User Defined Functions"},{"location":"streaming-runtime-overview/","text":"The Streaming Runtime ( SR ) implements, architecturally, the streaming data processing as a collection of independent event-driven streaming applications, called Processors , connected over a messaging middleware of choice (for example RabbitMQ or Apache Kafka). The connection between two or more Processors is called Stream : The Stream and the Processor 1 are implemented as Kubernetes API extensions, defined as Custom Resources and implementing Reconciliation Controllers for them. Consult the SR technical stack for further implementation details. The Streams CR instance specifies storage-at-rest of time-ordered attribute-partitioned , structured data, such as a Kafka topic, or RabbitMQ exchange/queue. This specification is used by the SR controllers to configure and wire the underlining connections. The Processor CR instance defines the manifest of the event-driven streaming application to be deployed by the SR processor controllers. Once deployed the application continuously receives input streaming data, transforms it and sends the results downstream over the outputs. The Processor can have zero or more input and output Streams . The collection of Processors and Streams come together at runtime to constitute streaming Data Pipelines (sometimes referred as Multistage topologies ): Simplified diagram Full (with ClusterStream) diagram The pipelines can be linear or nonlinear, depending on the data flows between the applications. After installing the SR operator, one can use the kind:Stream and kind:Processor resources to define a new streaming application 2 manifest: Development stage Production stage For convenience during the development stage, the SR operator auto-provisions the ClusterStreams for all Streams that don't have explicitly declared them. simple-streaming-app.yaml apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : data-in-stream spec : name : data-in protocol : \"kafka\" --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : multibinder-processor spec : type : SRP # use the built-in processor implementation inputs : - name : data-in-stream outputs : - name : data-out-stream --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : data-out-stream spec : name : data-out protocol : \"rabbitmq\" In production environment the Streaming Runtime will not be allowed to auto-provision ClusterStreams dynamically. Instead the Administrator will provision the required messaging middleware and declare ClusterStream to provide managed and controlled access for it. The ClusterStreams and the Streams follow the PersistentVolume model: namespaced Stream declared by a developer (ala PVC ) is backed by a ClusterStream resource (ala PV ) which is controlled and provisioned by the administrator. simple-streaming-app.yaml ################################################# # ADMIN responsibility ################################################# apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : ClusterStream metadata : name : kafka-cluster-stream spec : name : data-in streamModes : [ \"read\" , \"write\" ] storage : server : url : \"kafka.default.svc.cluster.local:9092\" protocol : \"kafka\" reclaimPolicy : \"Retain\" --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : ClusterStream metadata : name : rabbitmq-cluster-stream spec : name : data-out streamModes : [ \"read\" , \"write\" ] storage : server : url : \"rabbitmq.default.svc.cluster.local:5672\" protocol : \"rabbitmq\" reclaimPolicy : \"Retain\" --- ################################################# # DEVELOPER responsibility ################################################# apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : data-in-stream spec : name : data-in protocol : \"kafka\" storage : # Claims the pre-provisioned Kafka ClusterStream. clusterStream : kafka-cluster-stream --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : multibinder-processor spec : type : SRP inputs : - name : data-in-stream outputs : - name : data-out-stream --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : data-out-stream spec : name : data-out protocol : \"rabbitmq\" storage : # Claims the pre-provisioned rabbitmq ClusterStream. clusterStream : rabbitmq-cluster-stream and submit it to a Kubernetes cluster: kubectl apply -f ./simple-streaming-app.yaml -n streaming-runtime On submission, the SR controllers react by provisioning and configuring the specified resources. For example the SRP processor type, defined in the Processor CR , instructs the SR to provision the built-in, general purpose, SRP processor implementation. Likewise if the messaging middleware claimed by the Stream CRs (in this example: Apache Kafka and RabbitMQ) is not available, the controllers for the ClusterStreams will detect and auto-provision it. Developers have different options to build their own data transformation logic. Depending on the extension approach, processor are grouped in the following types: SRP Processor - is a general purpose, processor, that allows developers to plug in their own, polyglot User Defined Functions ( UDF ). The SRP processors provide support for Tumbling Time-Window aggregations and streaming Data Partitioning . SCS Processor - can run any Spring Cloud Stream application natively in Kubernetes. One can choose from the extensive set (60+) of pre-built streaming applications or build a custom one. The SCS Processor supports stateful and stateless workloads and stream data partitioning 3 . FSQL Processor - is backed by Apache Flink and supports streaming SQL . This allows the developers to implement complex data transformation, such as stream join or windowed aggregation, by defining streaming SQL queries. Developers can choose and mix the processor types interchangeably when implementing the streaming applications for their data pipelines. Next Steps After installing the streaming runtime, follow the Examples for various executable examples. The Streaming Runtime Operator provides also a ClusterStreams resources, that are responsible to provision the messaging middleware used by the Streams . The ClusterStreams and the Streams follow the PersistentVolume model: namespaced Stream declared by a developer (ala PVC ) is backed by a ClusterStream resource (ala PV ) which is controlled and provisioned by the administrator. For convenience during the development stage, the SR operator auto-provisions the ClusterStreams for all Streams that don't have explicitly declared them. \u21a9 The sample app itself acts as a message bridge. It receives input messages from Apache Kafka, data-in topic and re-transmits them, unchanged, to the output RabbitMQ data-out exchange. \u21a9 The SCS Processor also provides limited support for polyglot applications . \u21a9","title":"Overview"},{"location":"streaming-runtime-overview/#next-steps","text":"After installing the streaming runtime, follow the Examples for various executable examples. The Streaming Runtime Operator provides also a ClusterStreams resources, that are responsible to provision the messaging middleware used by the Streams . The ClusterStreams and the Streams follow the PersistentVolume model: namespaced Stream declared by a developer (ala PVC ) is backed by a ClusterStream resource (ala PV ) which is controlled and provisioned by the administrator. For convenience during the development stage, the SR operator auto-provisions the ClusterStreams for all Streams that don't have explicitly declared them. \u21a9 The sample app itself acts as a message bridge. It receives input messages from Apache Kafka, data-in topic and re-transmits them, unchanged, to the output RabbitMQ data-out exchange. \u21a9 The SCS Processor also provides limited support for polyglot applications . \u21a9","title":"Next Steps"},{"location":"architecture/cluster-streams/overview/","text":"Cluster Streams The Streaming Runtime Operator provides ClusterStreams allowing operators install dynamic Cluster Stream provisioners for developers to consume and create streams e.g. Kafka topics, or they may choose to limit creation of topics to administrators. ClusterStreams contains the information where the stream cluster is and its bindings. apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : ClusterStream metadata : {} spec : # Topic/Exchange name name : \"topicOrExchangeNme\" # Key attributes for the topic keys : [ <string> ] # Streaming modes that will be allowed at the creation of Streams e.g. read, write streamModes : [ <string> ] storage : # Information about the Message Broker to assess and manage. server : # Reference to an existing Service Binding Service (e.g. secrets). binding : <string> # Message Broker connection URL url : <string> # Message Broker type protocol : <string> reclaimPolicy : <string> attributes : # (optional) message-broker auto-provisioning adapter name protocolAdapterName : <auto-provisioning adapter name> For a detailed description of attributes of the resource please read cluster-stream-crd.yaml Stream relationship The ClusterStreams and the Streams follow the PersistentVolume model: namespaced Stream declared by a developer (ala PVC ) is backed by a ClusterStream resource (ala PV ) which is controlled and provisioned by the administrator. For convenience during the development stage, the SR operator auto-provisions the ClusterStreams for all Streams that don't have explicitly declared them. Service Binding The Service Binding Specification provides a Kubernetes-wide specification for communicating service secrets to workloads in an automated way. The Stream spec.binding allow to refer existing service binding service (aka secrets). Message Brokers Auto-provisioning The ClusterStream's controller uses the spec.storage.server information to connect to the target messaging broker (aka Apache Kafka, RabbitMQ and so.) and apply the configured policies. If the target messaging broker is missing, by default the ClusterStream controller will try to auto-provision one. This behavior is handy in the development stage where the developer is not bothered to provision the messaging infrastructure and instead lets the SR auto-provision and tear down the messaging infrastructure on demand. In production environments this behavior is likely to be disabled. Currently The ClusterStream Controllers can auto-provision: Apache Kafka with Schema Registry and Kafka UI console. RabbitMQ. You can use the simple rabbitmq deployment (default) or leverage the RabbitMQ Operator by setting protocolAdapterName attribute to rabbitmq-operator . RabbitMQ OperatorSample . Key Capabilities Ability to reference an already-existing stream to support interoperability with other systems Knowledge and documentation of the partitioning and schema of the stream data Ability to provision new streams and set the partition key Stream status should provide a Duck-type contract which provides all the necessary information to consume the stream once provisioned.","title":"Cluster Streams"},{"location":"architecture/cluster-streams/overview/#cluster-streams","text":"The Streaming Runtime Operator provides ClusterStreams allowing operators install dynamic Cluster Stream provisioners for developers to consume and create streams e.g. Kafka topics, or they may choose to limit creation of topics to administrators. ClusterStreams contains the information where the stream cluster is and its bindings. apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : ClusterStream metadata : {} spec : # Topic/Exchange name name : \"topicOrExchangeNme\" # Key attributes for the topic keys : [ <string> ] # Streaming modes that will be allowed at the creation of Streams e.g. read, write streamModes : [ <string> ] storage : # Information about the Message Broker to assess and manage. server : # Reference to an existing Service Binding Service (e.g. secrets). binding : <string> # Message Broker connection URL url : <string> # Message Broker type protocol : <string> reclaimPolicy : <string> attributes : # (optional) message-broker auto-provisioning adapter name protocolAdapterName : <auto-provisioning adapter name> For a detailed description of attributes of the resource please read cluster-stream-crd.yaml","title":"Cluster Streams"},{"location":"architecture/cluster-streams/overview/#stream-relationship","text":"The ClusterStreams and the Streams follow the PersistentVolume model: namespaced Stream declared by a developer (ala PVC ) is backed by a ClusterStream resource (ala PV ) which is controlled and provisioned by the administrator. For convenience during the development stage, the SR operator auto-provisions the ClusterStreams for all Streams that don't have explicitly declared them.","title":"Stream relationship"},{"location":"architecture/cluster-streams/overview/#service-binding","text":"The Service Binding Specification provides a Kubernetes-wide specification for communicating service secrets to workloads in an automated way. The Stream spec.binding allow to refer existing service binding service (aka secrets).","title":"Service Binding"},{"location":"architecture/cluster-streams/overview/#message-brokers-auto-provisioning","text":"The ClusterStream's controller uses the spec.storage.server information to connect to the target messaging broker (aka Apache Kafka, RabbitMQ and so.) and apply the configured policies. If the target messaging broker is missing, by default the ClusterStream controller will try to auto-provision one. This behavior is handy in the development stage where the developer is not bothered to provision the messaging infrastructure and instead lets the SR auto-provision and tear down the messaging infrastructure on demand. In production environments this behavior is likely to be disabled. Currently The ClusterStream Controllers can auto-provision: Apache Kafka with Schema Registry and Kafka UI console. RabbitMQ. You can use the simple rabbitmq deployment (default) or leverage the RabbitMQ Operator by setting protocolAdapterName attribute to rabbitmq-operator . RabbitMQ OperatorSample .","title":"Message Brokers Auto-provisioning"},{"location":"architecture/cluster-streams/overview/#key-capabilities","text":"Ability to reference an already-existing stream to support interoperability with other systems Knowledge and documentation of the partitioning and schema of the stream data Ability to provision new streams and set the partition key Stream status should provide a Duck-type contract which provides all the necessary information to consume the stream once provisioned.","title":"Key Capabilities"},{"location":"architecture/data-partitioning/data-partitioning/","text":"Data Partitioning Info Applicable for the SRP and SCS processor types. The FSQL processors use the Apache Flink built-in data partitioning capabilities. Partitioning is an essential concept in stateful data processing. It permits consistent data scaling by ensuring that all related data is processed together. Logically it implements the group by key processing. For example, in a time-windowed average calculation example, all measurements from any given sensor are processed by the same Processor instance. Technically, the partitioning allows content-based routing of payloads to the downstream processor instances. This is especially useful when you want to have your downstream processor instances to process data from specific partitions from the upstream processor. For instance, if a processor application in the data pipeline is performing operations based on a unique identifier from the payload (such as team name), the stream can be partitioned based on that unique identity. Some messaging middleware (such as Apache Kafka) provide additional guarantees that the data in the partitions remains ordered! The Streaming Runtime provides a simple construct to enable and configure stateful data partitioning. On the Steam resource that represents the partitioned connection, use the spec.key or spec.keyExpression to define the what header or payload field to use as a discriminator to partition the data in the steam. Additionally use the spec.partitionCount property to configure the number of partitions you would like the incoming data to be partitioned into. Those properties are used to instruct the upstream processor(s) to provision the data partitioning configuration while the downstream processors are configured for partitioned inputs (e.g. enforce instance ordering and stateful connections). If the downstream processor is scaled out (e.g. replications: N ), then the streaming runtime will ensure StatefulSet replication instead of Deployment/ReplicationSet . Additionally, for the processors consuming partitioned Stream, the SR configures Pod's Ordinal Index to be used as partition instance-index. Later ensures that event after Pod failure/restart the same partitions will be (re)assigned to it. For example let\u2019s consider an online-game statistics use case where we want to partition the user data by the team field. Using the SR constructs we can define pipeline like this: The input (data-in) contains user gaming statistics such as score, and team the user is a member of. This data is fed into the user-partition Processor and in turn sent downstream via the partition-by-team Steam configured to partition the data by team name into 3 partition groups. The downstream (user-score-processor) consumes the partitioned input and because it deploys 3 stateful instances we could expect that each partition will be assigned to a single processor instance. Learn how to build partitioned time-window aggregations with the SRP processor. Also visit the following example to learn how to define and configure partitioned streaming pipelines: 5-partition-by-field-with-stateful-replication.yaml 5.1-partition-by-field(header-keys)-with-stateful-replication.yaml 6.1-partition-by-field-stateful-replication-time-window-aggregation.yaml online-gaming-statistics","title":"Data Partitioning"},{"location":"architecture/data-partitioning/data-partitioning/#data-partitioning","text":"Info Applicable for the SRP and SCS processor types. The FSQL processors use the Apache Flink built-in data partitioning capabilities. Partitioning is an essential concept in stateful data processing. It permits consistent data scaling by ensuring that all related data is processed together. Logically it implements the group by key processing. For example, in a time-windowed average calculation example, all measurements from any given sensor are processed by the same Processor instance. Technically, the partitioning allows content-based routing of payloads to the downstream processor instances. This is especially useful when you want to have your downstream processor instances to process data from specific partitions from the upstream processor. For instance, if a processor application in the data pipeline is performing operations based on a unique identifier from the payload (such as team name), the stream can be partitioned based on that unique identity. Some messaging middleware (such as Apache Kafka) provide additional guarantees that the data in the partitions remains ordered! The Streaming Runtime provides a simple construct to enable and configure stateful data partitioning. On the Steam resource that represents the partitioned connection, use the spec.key or spec.keyExpression to define the what header or payload field to use as a discriminator to partition the data in the steam. Additionally use the spec.partitionCount property to configure the number of partitions you would like the incoming data to be partitioned into. Those properties are used to instruct the upstream processor(s) to provision the data partitioning configuration while the downstream processors are configured for partitioned inputs (e.g. enforce instance ordering and stateful connections). If the downstream processor is scaled out (e.g. replications: N ), then the streaming runtime will ensure StatefulSet replication instead of Deployment/ReplicationSet . Additionally, for the processors consuming partitioned Stream, the SR configures Pod's Ordinal Index to be used as partition instance-index. Later ensures that event after Pod failure/restart the same partitions will be (re)assigned to it. For example let\u2019s consider an online-game statistics use case where we want to partition the user data by the team field. Using the SR constructs we can define pipeline like this: The input (data-in) contains user gaming statistics such as score, and team the user is a member of. This data is fed into the user-partition Processor and in turn sent downstream via the partition-by-team Steam configured to partition the data by team name into 3 partition groups. The downstream (user-score-processor) consumes the partitioned input and because it deploys 3 stateful instances we could expect that each partition will be assigned to a single processor instance. Learn how to build partitioned time-window aggregations with the SRP processor. Also visit the following example to learn how to define and configure partitioned streaming pipelines: 5-partition-by-field-with-stateful-replication.yaml 5.1-partition-by-field(header-keys)-with-stateful-replication.yaml 6.1-partition-by-field-stateful-replication-time-window-aggregation.yaml online-gaming-statistics","title":"Data Partitioning"},{"location":"architecture/processors/overview/","text":"Processors The Processor represents an independent event-driven streaming application that can consume one or more input Streams, transform the received data and send the results downstream over one or more output Streams. For a detailed description of attributes of the resource please read processor-crd.yaml The Streaming Runtime provides a built-in, general purpose Processor of type SRP and to additional processor types to provide integration with 3rd party streaming technologies, such as Apache Flink (type: FSQL ) and Spring Cloud Stream/Spring Cloud Function (type: SCS ). Processors from all types can be combined and used interchangeably. The Streaming Runtime allows implementing additional Processor types that can provide integration with other streaming systems such as Apache Spark, KSQL and alike. Processor types SRP : Streaming Runtime Processor. Processor built-in the Streaming Runtime, that allow various streaming transformation, such as message brokers bridging, custom user-defined functions in the language of choice and simple tumbling time-window aggregation. FSQL : Backed by Apache Flink SQL Streaming. Allow inline streaming SQL queries definition. SCS : Runs Spring Cloud Stream applications as processors in the pipeline.","title":"Overview"},{"location":"architecture/processors/overview/#processors","text":"The Processor represents an independent event-driven streaming application that can consume one or more input Streams, transform the received data and send the results downstream over one or more output Streams. For a detailed description of attributes of the resource please read processor-crd.yaml The Streaming Runtime provides a built-in, general purpose Processor of type SRP and to additional processor types to provide integration with 3rd party streaming technologies, such as Apache Flink (type: FSQL ) and Spring Cloud Stream/Spring Cloud Function (type: SCS ). Processors from all types can be combined and used interchangeably. The Streaming Runtime allows implementing additional Processor types that can provide integration with other streaming systems such as Apache Spark, KSQL and alike.","title":"Processors"},{"location":"architecture/processors/overview/#processor-types","text":"SRP : Streaming Runtime Processor. Processor built-in the Streaming Runtime, that allow various streaming transformation, such as message brokers bridging, custom user-defined functions in the language of choice and simple tumbling time-window aggregation. FSQL : Backed by Apache Flink SQL Streaming. Allow inline streaming SQL queries definition. SCS : Runs Spring Cloud Stream applications as processors in the pipeline.","title":"Processor types"},{"location":"architecture/processors/fsql/overview/","text":"Apache Flink SQL Processor ( FSQL ) Backed by Apache Flink SQL Streaming, it allows inline Query definitions to be expressed in the resource. The set of input stream data which should trigger a transformation is represented by a (streaming) SQL query across the various inputs which yields event tuples which are emitted to the output streams. Usage apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : {} spec : # Type of the processor. In this case FSQL type : FSQL # SQL query that will be executed. inlineQuery : - <string sql> attributes : # TODO: explain this debugQuery : \"SELECT * FROM PossibleFraud\" # TODO: explain this debugExplain : \"2\" Examples Anomaly Detection ( FSQL , SRP )- detect, in real time, suspicious credit card transactions, and extract them for further processing. Clickstream Analysis ( FSQL , SRP ) - for an input clickstream stream, we want to know who are the high status customers, currently using the website so that we can engage with them or to find how much they buy or how long they stay on the site that day. IoT Monitoring analysis ( FSQL , SRP ) - real-time analysis of IoT monitoring log. Streaming Music Service ( FSQL , SRP ) - music ranking application that continuously computes the latest Top-K music charts based on song play events collected in real-time.","title":"FSQL Processor"},{"location":"architecture/processors/fsql/overview/#apache-flink-sql-processor-fsql","text":"Backed by Apache Flink SQL Streaming, it allows inline Query definitions to be expressed in the resource. The set of input stream data which should trigger a transformation is represented by a (streaming) SQL query across the various inputs which yields event tuples which are emitted to the output streams.","title":"Apache Flink SQL Processor (FSQL)"},{"location":"architecture/processors/fsql/overview/#usage","text":"apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : {} spec : # Type of the processor. In this case FSQL type : FSQL # SQL query that will be executed. inlineQuery : - <string sql> attributes : # TODO: explain this debugQuery : \"SELECT * FROM PossibleFraud\" # TODO: explain this debugExplain : \"2\"","title":"Usage"},{"location":"architecture/processors/fsql/overview/#examples","text":"Anomaly Detection ( FSQL , SRP )- detect, in real time, suspicious credit card transactions, and extract them for further processing. Clickstream Analysis ( FSQL , SRP ) - for an input clickstream stream, we want to know who are the high status customers, currently using the website so that we can engage with them or to find how much they buy or how long they stay on the site that day. IoT Monitoring analysis ( FSQL , SRP ) - real-time analysis of IoT monitoring log. Streaming Music Service ( FSQL , SRP ) - music ranking application that continuously computes the latest Top-K music charts based on song play events collected in real-time.","title":"Examples"},{"location":"architecture/processors/scs/overview/","text":"Spring Cloud Stream Processor ( SCS ) Runs Spring Cloud Stream applications as processors in the pipeline. One can choose for the extensive set (60+) of pre-built streaming applications or build a custom one. It is possible to build and deploy polyglot applications as long as they interact with the input/output streams manually. Usage apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : # Name of the source of the Spring Cloud Stream. # List: https://docs.spring.io/stream-applications/docs/2021.1.2/reference/html/#sources name : <string> spec : # Type of the processor. In this case SCS (Spring Cloud Stream) type : SCS # Input Stream name for the processor to get data from inputs : - name : <string> # Output Stream name for the processor to send data outputs : - name : <string> template : spec : # Container for the Spring Cloud Stream image. containers : - name : <string> image : springcloudstream/<processor>:<tag> # List of environment variables that are required for the processor. env : Examples Spring Cloud Stream pipeline ( SCS ) - show how to build streaming pipelines using Spring Cloud Stream application as processors. streaming-pipeline-ticktock-partitioned-better.yaml example shows how to data-partition the TickTock application leveraging the SCS Data-Partitioning capabilities.","title":"SCS Processor"},{"location":"architecture/processors/scs/overview/#spring-cloud-stream-processor-scs","text":"Runs Spring Cloud Stream applications as processors in the pipeline. One can choose for the extensive set (60+) of pre-built streaming applications or build a custom one. It is possible to build and deploy polyglot applications as long as they interact with the input/output streams manually.","title":"Spring Cloud Stream Processor (SCS)"},{"location":"architecture/processors/scs/overview/#usage","text":"apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : # Name of the source of the Spring Cloud Stream. # List: https://docs.spring.io/stream-applications/docs/2021.1.2/reference/html/#sources name : <string> spec : # Type of the processor. In this case SCS (Spring Cloud Stream) type : SCS # Input Stream name for the processor to get data from inputs : - name : <string> # Output Stream name for the processor to send data outputs : - name : <string> template : spec : # Container for the Spring Cloud Stream image. containers : - name : <string> image : springcloudstream/<processor>:<tag> # List of environment variables that are required for the processor. env :","title":"Usage"},{"location":"architecture/processors/scs/overview/#examples","text":"Spring Cloud Stream pipeline ( SCS ) - show how to build streaming pipelines using Spring Cloud Stream application as processors. streaming-pipeline-ticktock-partitioned-better.yaml example shows how to data-partition the TickTock application leveraging the SCS Data-Partitioning capabilities.","title":"Examples"},{"location":"architecture/processors/srp/overview/","text":"Streaming Runtime Processor ( SRP ) The SRP processor is built-in the Streaming Runtime, provide common capabilities such as message brokerage, inline streaming transformations , polyglot user-defined functions , simple tumbling time-window aggregation and data-partitioning capabilities to name a few. Message Transformation Options The SRP offers 4 ways to implement event transformation logic: Message Broker Bridge If no inline transformation or UDF functions are configured, the SRP processor simply retransmits the inbound events, unchanged, to the outbound Streams. This could be useful to implement Message Broker Bridges . Inline Transformations The srp.spel.expression attribute allows setting an inline, SpEL expressions as a transformation function. SpEL is a powerful expression language that supports querying and manipulating an messages at runtime. As Message format has two parts ( headers and payload ) that allow SpEL expressions such as payload , payload.thing , headers['my.header'] , and so on. The inline-transformation example shows how to apply JsonPath expressions to transform the inbound JSON payload. Header Enrichment The srp.output.headers attribute allows enriching the outbound message headers with values computed form the inbound message header or payload. For example the srp.output.headers: \"user=payload.fullName\" expression would add a new outbound header named user with value computed from the inbound payload field fullName . Expression support any structured payload data formats such as Json, AVRO and so on. User Defined Functions (Polyglot) Polyglot User Defined Function ( UDF ) . The Streaming Runtime allows implementing the message transformation logic in the language of your choice, packaged in a standalone image container and deployed as a sidecar in the same Pod along with the SRP Processor. The communication between the SRP Processor and the UDF running in the sidecar is over gRPC and uses a well defined ProtocolBuffer Message Service interface: Time-Window Aggregation Capability For simple workloads the SRP Processor offers lighter, configurable tumbling time-window capabilities with in-memory or local state persistence. A tumbling time-window assigns each message to a window of a specified time interval. Tumbling windows have a fixed size and do not overlap. Find detailed description of SRP Time-Window capabilities . Streaming Data Partitioning Common Streaming Data Partitioning Support for SRP and SCS Processor types.","title":"Overview"},{"location":"architecture/processors/srp/overview/#streaming-runtime-processor-srp","text":"The SRP processor is built-in the Streaming Runtime, provide common capabilities such as message brokerage, inline streaming transformations , polyglot user-defined functions , simple tumbling time-window aggregation and data-partitioning capabilities to name a few.","title":"Streaming Runtime Processor (SRP)"},{"location":"architecture/processors/srp/overview/#message-transformation-options","text":"The SRP offers 4 ways to implement event transformation logic:","title":"Message Transformation Options"},{"location":"architecture/processors/srp/overview/#message-broker-bridge","text":"If no inline transformation or UDF functions are configured, the SRP processor simply retransmits the inbound events, unchanged, to the outbound Streams. This could be useful to implement Message Broker Bridges .","title":"Message Broker Bridge"},{"location":"architecture/processors/srp/overview/#inline-transformations","text":"The srp.spel.expression attribute allows setting an inline, SpEL expressions as a transformation function. SpEL is a powerful expression language that supports querying and manipulating an messages at runtime. As Message format has two parts ( headers and payload ) that allow SpEL expressions such as payload , payload.thing , headers['my.header'] , and so on. The inline-transformation example shows how to apply JsonPath expressions to transform the inbound JSON payload.","title":"Inline Transformations"},{"location":"architecture/processors/srp/overview/#header-enrichment","text":"The srp.output.headers attribute allows enriching the outbound message headers with values computed form the inbound message header or payload. For example the srp.output.headers: \"user=payload.fullName\" expression would add a new outbound header named user with value computed from the inbound payload field fullName . Expression support any structured payload data formats such as Json, AVRO and so on.","title":"Header Enrichment"},{"location":"architecture/processors/srp/overview/#user-defined-functions-polyglot","text":"Polyglot User Defined Function ( UDF ) . The Streaming Runtime allows implementing the message transformation logic in the language of your choice, packaged in a standalone image container and deployed as a sidecar in the same Pod along with the SRP Processor. The communication between the SRP Processor and the UDF running in the sidecar is over gRPC and uses a well defined ProtocolBuffer Message Service interface:","title":"User Defined Functions (Polyglot)"},{"location":"architecture/processors/srp/overview/#time-window-aggregation-capability","text":"For simple workloads the SRP Processor offers lighter, configurable tumbling time-window capabilities with in-memory or local state persistence. A tumbling time-window assigns each message to a window of a specified time interval. Tumbling windows have a fixed size and do not overlap. Find detailed description of SRP Time-Window capabilities .","title":"Time-Window Aggregation Capability"},{"location":"architecture/processors/srp/overview/#streaming-data-partitioning","text":"Common Streaming Data Partitioning Support for SRP and SCS Processor types.","title":"Streaming Data Partitioning"},{"location":"architecture/processors/srp/time-window-aggregation/","text":"Some data transformations, such as group-by-key, aggregate multiple messages by a common key. For bounded (e.g. batch) datasets, those operations group all of the messages with the same key within the entire data set. But it is impossible in unbounded datasets (e.g. streaming), where new records are being ingested infinitely. Consequently, such types of data workloads are commonly processed in windows. Any unbounded dataset can be divided into logical windows. Each message received from the ubound datasets is assigned to one or more windows according to some windowing function, ensuring that each individual window contains a finite number of messages. Grouping transformations then can be applied on each message on a per-window basis. For example group-by-key groups the records within a time-window. The Streaming Runtime offers two approaches for data windowing. For demanding, complex data processing, you should consider applying the FSQL Processor type, which provides integration with the powerful Apache Flink Streaming SQL . The FSQL samples demonstrate how to leverage this approach. For simple workload the SRP Processor offers lighter, configurable tumbling time-window capabilities with in-memory or local state persistence. A tumbling time-window assigns each message to a window of a specified time interval. Tumbling windows have a fixed size and do not overlap. For example, if you specify a tumbling window with a size of 5 minutes. This feature is useful for workloads where you need to calculate aggregates continuously. For example, for a retailer streaming order information from different remote stores, it can generate near real-time sales statistics for downstream processing. The SRP Processor\u2019s time-window aggregates functionality is commonly used in combination with Aggregate UDFs that operate on the aggregated window data. When enabled the tumbling window functionality groups the inbound messages in windowed aggregates and sends later to the configured, Aggregation UDF . The UDF function returns a value or multiple values that are sent downstream for further processing. To enable the tumbling window, you need to set the window duration with the SRP Processor srp.window attribute . This instructs the processor to collect the inbound messages into time-window groups based on a event-time computed for every message. The inbound Stream\u2019s spec.timeAttributes defines how the message event-time is computed and via the watermark expression, how to generate the input watermarks. Even-Time vs Processing Time When processing data which relate to events in time, there are two inherent domains of time to consider: (1) Event Time, which is the time at which the event itself actually occurred and (2) Processing Time, which is the time at which an event is observed at any given point during processing within the pipeline, i.e. the current time according to the system clock. Event time for a given event essentially never changes, but processing time changes constantly for each event as it flows through the pipeline and time marches ever forward. This is an important distinction when it comes to robustly analysing events in the context of when they occurred. The Streaming Runtime can compute event-times from the inbound message body or metadata (e.g. Kafka timestamp header) and defaults to processing time if no time attributes are specified. The Watermarks , implemented by the SRP Processor , is an essential tool for reasoning about temporal completeness in infinite streams. It provides a practical way to estimate when a time-window aggregate should have received all its messages and can be sent to the UDF . In essence the watermark toolkit helps to propagate consistently the event-time (e.g. the time event occurred in its business domain) as the input events (e.g. messages) get processed through the streaming pipeline (at processing time). For example, let's construct a simple streaming analytics pipeline for a multi-platform team game, aggregating every 15 seconds, per-team scores. Let\u2019s assume that the input data has a format like this: User <name: string, team: (red or blue), score:int, time_score: timestamp> Then we can configure the SR pipeline to use the time_score field as event-time and a watermark to tolerate a few seconds out-of-orderness. Then set the processor with srp.window interval of 15 seconds and references to the polyglot Aggregation UDF that computes the results from the window aggregates. For example we can implement a simple Node.js UDF that groups the scores per team in every window. The Stream and Processor definitions for this example might look like this: You can find here the actual time-window aggregation example data-pipeline: 6-time-window-aggregation.yaml Without getting into details about how the watermarks work internally, here is an approximation how the inbound messages are grouped in time-window aggregates, which after processed by the UDF produces new outbound messages sent downstream: When deployed the pipeline would look like this: Partitioned Time-window Aggregation The time-window aggregation collects, locally, temporal groups of messages (e.g. windows) that could put pressure on Pod\u2019s memory. Also the aggregation tasks are often CPU demanding and can become a performance bottleneck. To alleviate those problems we can use the processor replications property to spin multiple Processor instances that do the aggregation in parallel. Because this is an aggregation Process the SR operator would make sure to run the new instances into stateful (aka SatefulSet ) containers. But just increasing the number of Processor instances would likely lead to incorrect aggregation results. Since the aggregation task performs group-by-key it is critical that the same key for a given time-window interval is always sent to the same Processor instance! Fortunately Data Partitioning provides this capability! If we partition the inbound Stream on the same key used for grouping we can ensure that the message middleware and Kubernetes state management will always dispatch the messages with the same keys to the same processor instance. In short we can reliably scale out the time-window aggregation processors by ensuring inbound data partitioning on the same key! So our scaled out multiplatform-game application will look like this: Explore the complete 6.1-partition-by-field-stateful-replication-time-window-aggregation.yaml example.","title":"Time-Window Aggregation"},{"location":"architecture/processors/srp/time-window-aggregation/#partitioned-time-window-aggregation","text":"The time-window aggregation collects, locally, temporal groups of messages (e.g. windows) that could put pressure on Pod\u2019s memory. Also the aggregation tasks are often CPU demanding and can become a performance bottleneck. To alleviate those problems we can use the processor replications property to spin multiple Processor instances that do the aggregation in parallel. Because this is an aggregation Process the SR operator would make sure to run the new instances into stateful (aka SatefulSet ) containers. But just increasing the number of Processor instances would likely lead to incorrect aggregation results. Since the aggregation task performs group-by-key it is critical that the same key for a given time-window interval is always sent to the same Processor instance! Fortunately Data Partitioning provides this capability! If we partition the inbound Stream on the same key used for grouping we can ensure that the message middleware and Kubernetes state management will always dispatch the messages with the same keys to the same processor instance. In short we can reliably scale out the time-window aggregation processors by ensuring inbound data partitioning on the same key! So our scaled out multiplatform-game application will look like this: Explore the complete 6.1-partition-by-field-stateful-replication-time-window-aggregation.yaml example.","title":"Partitioned Time-window Aggregation"},{"location":"architecture/processors/srp/udf-build/","text":"The udf-uppercase-java , udf-uppercase-go , and udf-uppercase-python sample projects show how to build simple UDFs in Java , Python or Go using the Reques/Repply RPC mode. Also, you can find there instructions how to build the UDF container image and push those to the container registry of choice. For example in case of the Python UDF you can use a Dockerfile like this: FROM python:3.9.7-slim RUN pip install grpcio RUN pip install grpcio-tools ADD MessageService_pb2.py / ADD MessageService_pb2_grpc.py / ADD message_service_server.py / ENTRYPOINT [ \"python\" , \"/message_service_server.py\" ] CMD [] to build the container image: docker build -t ghcr.io/vmware-tanzu/streaming-runtimes/udf-uppercase-python:0.1 . and push it to the registry: docker push ghcr.io/vmware-tanzu/streaming-runtimes/udf-uppercase-python:0.1 Then you can refer this image from within your streaming Processor CR definitions. For example: 1. apiVersion : streaming.tanzu.vmware.com/v1alpha1 2. kind : Processor 3. metadata : 4. name : my-streaming-processor 5. spec : 6. inputs : 7. - name : \"my-input-stream\" # input streams for the UDF function 8. outputs : 9. - name : \"my-output-stream\" # output streams for the UDF function 10. template : 11. spec : 12. containers : 13. - name : my-python-udf-container 14. image : ghcr.io/vmware-tanzu/streaming-runtimes/udf-uppercase-python:0.1 Note that the my-python-udf-container (lines 13 - 14 ) uses the udf-uppercase-python:0.1 image. TODO: Add Aggregate UDF implementation instructions.","title":"Build UDF"},{"location":"architecture/processors/srp/udf-overview/","text":"User Defined Functions The Streaming Runtime provides a pluggable User Defined Functions ( UDF ) that allows implementing the streaming transformation logic in a language of your choice, test it in isolation with your favorite tools and finally package it in a standalone container image. The function is deployed as a sidecar along the SRP processor that acts as the connection between the streams and the function deployed. To build your custom function it should adhere to the UDF Protocol Buffer Contract and run as gRPC service. UDF Types Two types of UDF functions are supported: Mapping UDF and Aggregation UDF Mapping UDF - The SRP forwards the inbound messages, element-wise over the MessagingService , to the UDF function. The function computes a result, returns it to the SRP , that in turn sends it downstream. Every inbound message produces a single outbound result! The UDF image is registered with the SRP Processor using the spec under the spec.templates.spec.containers section. The SR will deploy the image in side-container in the same pod as the SRP Processor. Aggregation UDF - When the Time-Window aggregation is enabled and a window is ready for release, the SRP processor forwards the window content (e.g. collection of messages) to the UDF function. Later processes the collection, computes one or more aggregation results that are returned to the SRP and sent downstream. Resource Definition To plug a custom UDF to your SRP Processor, you can refer to UDF \u2019s image from within the Processor resourced definition: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : {} spec : # Processor Type: Streaming Runtime Processor (SRP) type : SRP # Name of the input stream to get data from inputs : - name : <string> # Name of the output stream to send data to outputs : - name : <string> attributes : # UDF gRPC connection port srp.grpcPort : \"50051\" template : spec : containers : # Container with the UDF function image - name : <your-udf-container-name> image : <udf-repository-uri> # Environment variables applied to the UDF at runtime env : - name : <string> value : <any> UDF Contract The contract of the function specifies a GrpcMessage schema to model the messages exchanged between the multibinder and the function and the MessagingService rpc service to interact with the UDF . The GrpcPayloadCollection is a temporal workaround to help serialize/deserialize collection on messages, for example the time-window aggregates, to and from single byte array. This allow the SRP to sends time-window aggregates to the UDFs using the same GrpcMessage format. syntax = \"proto3\" ; option java_multiple_files = true ; package org . springframework.cloud.function.grpc ; message GrpcMessage { bytes payload = 1 ; map < string , string > headers = 2 ; } message GrpcPayloadCollection { repeated bytes payload = 1 ; } service MessagingService { rpc requestReply ( GrpcMessage ) returns ( GrpcMessage ); } The MessageService.proto allows you to generate required stubs to support the true polyglot nature of gRPC while interacting with functions hosted by Streaming Runtime . The SRP Processor forwards the incoming messages over the MessagingService to the pre-configured UDF function. The function response in turn is sent to the SRP 's output stream. Mapping UDF The Mapping UDF function runs a gRPC server with the MessagingService implementation. As shown in the following diagram, the SRP processor converts every inbound SR message into a GrpcMessage and invokes the requestReply method on the MessagingService . The UDF MessagingService#requestReply implementation, handles the invocation, computes a result and returns it back as GrpcMessage . The SRP processor converts the GrpcMessage result into internal SR Message and sends it downstream over the outbound Streams. The 3.1-polyglot-udf-transformation.yaml example, uses a simple Python mapping UDF to convert the payload to upper case. Following diagram visualizes how this polyglot-udf-transformation.yaml example is deployed by the Streaming Runtime into a running data pipeline: Processor's spec.templates.spec.containers properties are used to register the UDF 's image with the SRP processor to use it. Sidecar The Streaming RUntime collocates the UDF container along with the SPR processor container in the same Pod. This simplifies the (gRPC) communication between both containers as they use the 'localhost' network. Here a few snippets how to implement Mapping UDFs in different languages: Java: public Function < String , String > uppercase () { return v -> v . toUpperCase (); } You can find complete source code udf-uppercase-java . If you are building your Function in Java you can find more information about the Spring Cloud Function gRPC support here . Python: def requestReply ( self , request , context ): print ( \"Server received Payload: %s and Headers: %s \" % ( request . payload . decode (), request . headers )) return MessageService_pb2 . GrpcMessage ( payload = str . encode ( request . payload . decode () . upper ()), headers = request . headers ) You can find complete source code udf-uppercase-python GoLang: func ( s * server ) RequestReply ( ctx context . Context , in * pb . GrpcMessage ) ( * pb . GrpcMessage , error ) { log . Printf ( \"Received: %v\" , string ( in . Payload )) upperCasePayload := strings . ToUpper ( string ( in . Payload )) return & pb . GrpcMessage { Payload : [] byte ( upperCasePayload )}, nil } You can find complete source code udf-uppercase-go Aggregation UDF When the time-window aggregation is used the SRP processor forwards to the UDF not just a single message but the collection of all messages members of a time-window aggregation. Reversely the UDF may return not just a single result but a collection of results that are treated as separate downstream messages. The MessagingService , used by the Mapping UDFs , expects a single GrpcMessage as input and single GrpcMessage as an output. So if we are to reuse the same gRPC service for Aggregation UDFs we need a workaround to allow serializing and deserializing collection of SR Messages to and from single GrpcMessage . Furthermore we need to do it in interoperable (e.g. language neutral) fashion. The GrpcPayloadCollection message format is used to ensure interoperability of serialization and deserialization of the payloads for the messages exchanged between the SR Processor and the Aggregation UDF . Following diagram illustrates the message flow: The SR Message collections (aka time-window) is converted into a single GrpcMessage . The headers of the first SR Message in the window is used as headers for the GrpcMessage, including a hardcoded contentType header of type multipart/<inner-message-content-type> . All SR Message payloads in the window are serialized, with the GrpcPayloadCollection help, into a single byte array used as GrpcMessage payload. The Aggregation UDF is required to deserialize the GrpcMessage payload back into a collection of the original payloads, then apply the aggregation transformation and serialize the collection or results into a single byte array passed as payload in the return GrpcMessage. Finally the SRP Processor turns the returned GrpcMessage into collection of SR Messages and sends them down streams, one by one. The udf-utilities offers some helpers library that help to hide the gRPC and SerDeser boilerplate code. Note The GrpcPayloadCollection serialization/deserialization approach is a hackish workaround to reuse the existing MessagingService applicable for non-aggregated messages exchange. A proper, cleaner approach would be to implement a dedicated AggregatedMessagingService that takes a collection of GrpcMessage messages as input and output. Check the Time-Window Aggregation to see how Aggregation UDFs are being used to compute group-by-key results.","title":"User Defined Functions"},{"location":"architecture/processors/srp/udf-overview/#user-defined-functions","text":"The Streaming Runtime provides a pluggable User Defined Functions ( UDF ) that allows implementing the streaming transformation logic in a language of your choice, test it in isolation with your favorite tools and finally package it in a standalone container image. The function is deployed as a sidecar along the SRP processor that acts as the connection between the streams and the function deployed. To build your custom function it should adhere to the UDF Protocol Buffer Contract and run as gRPC service.","title":"User Defined Functions"},{"location":"architecture/processors/srp/udf-overview/#udf-types","text":"Two types of UDF functions are supported: Mapping UDF and Aggregation UDF Mapping UDF - The SRP forwards the inbound messages, element-wise over the MessagingService , to the UDF function. The function computes a result, returns it to the SRP , that in turn sends it downstream. Every inbound message produces a single outbound result! The UDF image is registered with the SRP Processor using the spec under the spec.templates.spec.containers section. The SR will deploy the image in side-container in the same pod as the SRP Processor. Aggregation UDF - When the Time-Window aggregation is enabled and a window is ready for release, the SRP processor forwards the window content (e.g. collection of messages) to the UDF function. Later processes the collection, computes one or more aggregation results that are returned to the SRP and sent downstream.","title":"UDF Types"},{"location":"architecture/processors/srp/udf-overview/#resource-definition","text":"To plug a custom UDF to your SRP Processor, you can refer to UDF \u2019s image from within the Processor resourced definition: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : {} spec : # Processor Type: Streaming Runtime Processor (SRP) type : SRP # Name of the input stream to get data from inputs : - name : <string> # Name of the output stream to send data to outputs : - name : <string> attributes : # UDF gRPC connection port srp.grpcPort : \"50051\" template : spec : containers : # Container with the UDF function image - name : <your-udf-container-name> image : <udf-repository-uri> # Environment variables applied to the UDF at runtime env : - name : <string> value : <any>","title":"Resource Definition"},{"location":"architecture/processors/srp/udf-overview/#udf-contract","text":"The contract of the function specifies a GrpcMessage schema to model the messages exchanged between the multibinder and the function and the MessagingService rpc service to interact with the UDF . The GrpcPayloadCollection is a temporal workaround to help serialize/deserialize collection on messages, for example the time-window aggregates, to and from single byte array. This allow the SRP to sends time-window aggregates to the UDFs using the same GrpcMessage format. syntax = \"proto3\" ; option java_multiple_files = true ; package org . springframework.cloud.function.grpc ; message GrpcMessage { bytes payload = 1 ; map < string , string > headers = 2 ; } message GrpcPayloadCollection { repeated bytes payload = 1 ; } service MessagingService { rpc requestReply ( GrpcMessage ) returns ( GrpcMessage ); } The MessageService.proto allows you to generate required stubs to support the true polyglot nature of gRPC while interacting with functions hosted by Streaming Runtime . The SRP Processor forwards the incoming messages over the MessagingService to the pre-configured UDF function. The function response in turn is sent to the SRP 's output stream.","title":"UDF Contract"},{"location":"architecture/processors/srp/udf-overview/#mapping-udf","text":"The Mapping UDF function runs a gRPC server with the MessagingService implementation. As shown in the following diagram, the SRP processor converts every inbound SR message into a GrpcMessage and invokes the requestReply method on the MessagingService . The UDF MessagingService#requestReply implementation, handles the invocation, computes a result and returns it back as GrpcMessage . The SRP processor converts the GrpcMessage result into internal SR Message and sends it downstream over the outbound Streams. The 3.1-polyglot-udf-transformation.yaml example, uses a simple Python mapping UDF to convert the payload to upper case. Following diagram visualizes how this polyglot-udf-transformation.yaml example is deployed by the Streaming Runtime into a running data pipeline: Processor's spec.templates.spec.containers properties are used to register the UDF 's image with the SRP processor to use it. Sidecar The Streaming RUntime collocates the UDF container along with the SPR processor container in the same Pod. This simplifies the (gRPC) communication between both containers as they use the 'localhost' network. Here a few snippets how to implement Mapping UDFs in different languages: Java: public Function < String , String > uppercase () { return v -> v . toUpperCase (); } You can find complete source code udf-uppercase-java . If you are building your Function in Java you can find more information about the Spring Cloud Function gRPC support here . Python: def requestReply ( self , request , context ): print ( \"Server received Payload: %s and Headers: %s \" % ( request . payload . decode (), request . headers )) return MessageService_pb2 . GrpcMessage ( payload = str . encode ( request . payload . decode () . upper ()), headers = request . headers ) You can find complete source code udf-uppercase-python GoLang: func ( s * server ) RequestReply ( ctx context . Context , in * pb . GrpcMessage ) ( * pb . GrpcMessage , error ) { log . Printf ( \"Received: %v\" , string ( in . Payload )) upperCasePayload := strings . ToUpper ( string ( in . Payload )) return & pb . GrpcMessage { Payload : [] byte ( upperCasePayload )}, nil } You can find complete source code udf-uppercase-go","title":"Mapping UDF"},{"location":"architecture/processors/srp/udf-overview/#aggregation-udf","text":"When the time-window aggregation is used the SRP processor forwards to the UDF not just a single message but the collection of all messages members of a time-window aggregation. Reversely the UDF may return not just a single result but a collection of results that are treated as separate downstream messages. The MessagingService , used by the Mapping UDFs , expects a single GrpcMessage as input and single GrpcMessage as an output. So if we are to reuse the same gRPC service for Aggregation UDFs we need a workaround to allow serializing and deserializing collection of SR Messages to and from single GrpcMessage . Furthermore we need to do it in interoperable (e.g. language neutral) fashion. The GrpcPayloadCollection message format is used to ensure interoperability of serialization and deserialization of the payloads for the messages exchanged between the SR Processor and the Aggregation UDF . Following diagram illustrates the message flow: The SR Message collections (aka time-window) is converted into a single GrpcMessage . The headers of the first SR Message in the window is used as headers for the GrpcMessage, including a hardcoded contentType header of type multipart/<inner-message-content-type> . All SR Message payloads in the window are serialized, with the GrpcPayloadCollection help, into a single byte array used as GrpcMessage payload. The Aggregation UDF is required to deserialize the GrpcMessage payload back into a collection of the original payloads, then apply the aggregation transformation and serialize the collection or results into a single byte array passed as payload in the return GrpcMessage. Finally the SRP Processor turns the returned GrpcMessage into collection of SR Messages and sends them down streams, one by one. The udf-utilities offers some helpers library that help to hide the gRPC and SerDeser boilerplate code. Note The GrpcPayloadCollection serialization/deserialization approach is a hackish workaround to reuse the existing MessagingService applicable for non-aggregated messages exchange. A proper, cleaner approach would be to implement a dedicated AggregatedMessagingService that takes a collection of GrpcMessage messages as input and output. Check the Time-Window Aggregation to see how Aggregation UDFs are being used to compute group-by-key results.","title":"Aggregation UDF"},{"location":"architecture/service-binding/service-binding/","text":"Overview To fulfil its tasks, the Streaming Runtime ( SR ) interacts with external distributed systems such as Apache Kafka, RabbitMQ, Apache Flink and others. This implies that SR uses credentials to access those systems. Furthermore the SR internally exchanges status information between the managed ( ClusterStream , Stream and Processor ) resources and part of this information could include sensitive attributes as well. To avoid configuring and sharing sensitive attributes in plain text the SR needs a mechanism to encapsulate and securely share such attributes amongst its managed resources. The Service Binding Specification for Kubernetes is designed to address this problem by providing a Kubernetes-wide specification for communicating service secrets to workloads in an automated way. The ClusterStream CRD offers a dedicated storage.server.binding attribute that can be used to refer to an existing Service Binding Service (e.g. secrets). apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : ClusterStream metadata : name : test-clusterstream spec : name : my-exchange storage : server : binding : \"my-service-binding-ref\" url : \"http://localhost:8080\" protocol : \"rabbitmq\" The ClusterStream reconciler will detect this attribute and converted it into status.binding.name in compliance with the ProvisioningService specification. ... status : binding : name : \"my-service-binding-ref\" ... Later is picked by the Service Binding Operator . It lookups and enforces all ServiceBinding resources with service name matching the provide binding name. For example apiVersion : servicebinding.io/v1beta1 kind : ServiceBinding metadata : name : streaming-runtime-rabbitmq spec : service : apiVersion : v1 kind : Secret name : my-service-binding-ref workload : apiVersion : apps/v1 kind : Deployment name : streaming-runtime-processor-possible-fraud-processor Enable Service Binding Install the Service Binding Operator . Any specification compliant operator can be used but we advice for the VMWare-Tanzu operator: kubectl apply -f https://github.com/vmware-tanzu/servicebinding/releases/download/v0.7.1/service-bindings-0.7.1.yaml Create Kubernetes Secrets for the protected services (e.g. Kafka, RabbitMQ \u2026). Note: When operators are used to provision those services, later create the needed secrets automatically. Follow the service operator instructions to find the names of the generated secrets. apiVersion : v1 kind : Secret metadata : name : streaming-runtime-rabbitmq-secret type : servicebinding.io/rabbitmq stringData : type : rabbitmq provider : rabbitmq host : rabbitmq.default.svc.cluster.local port : \"5672\" # demo credentials username : guest password : guest Create Service Binding contracts to bind the desired services and workloads. apiVersion : servicebinding.io/v1beta1 kind : ServiceBinding metadata : name : streaming-runtime-rabbitmq spec : service : apiVersion : v1 kind : Secret name : streaming-runtime-rabbitmq-secret workload : apiVersion : apps/v1 kind : Deployment name : streaming-runtime-processor-possible-fraud-processor env : - name : SPRING_RABBITMQ_PASSWORD key : password - name : SPRING_RABBITMQ_USERNAME key : username Add binding attribute to Stream resource to refer the service secret name apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : udf-output-possible-fraud-stream spec : name : udf-output-possible-fraud keys : [ \"card_id\" ] streamMode : [ \"write\" ] protocol : \"rabbitmq\" # Binding refers to a Secret with the same name. The stream controller uses this binding to configure ClusterStream's auto-creation binding : \"streaming-runtime-rabbitmq-secret\" storage : clusterStream : \"udf-output-possible-fraud-cluster-stream\" Future Work The Streaming Runtime should be able to create and manage the ServiceBinding objects internally","title":"Service Binding"},{"location":"architecture/service-binding/service-binding/#overview","text":"To fulfil its tasks, the Streaming Runtime ( SR ) interacts with external distributed systems such as Apache Kafka, RabbitMQ, Apache Flink and others. This implies that SR uses credentials to access those systems. Furthermore the SR internally exchanges status information between the managed ( ClusterStream , Stream and Processor ) resources and part of this information could include sensitive attributes as well. To avoid configuring and sharing sensitive attributes in plain text the SR needs a mechanism to encapsulate and securely share such attributes amongst its managed resources. The Service Binding Specification for Kubernetes is designed to address this problem by providing a Kubernetes-wide specification for communicating service secrets to workloads in an automated way. The ClusterStream CRD offers a dedicated storage.server.binding attribute that can be used to refer to an existing Service Binding Service (e.g. secrets). apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : ClusterStream metadata : name : test-clusterstream spec : name : my-exchange storage : server : binding : \"my-service-binding-ref\" url : \"http://localhost:8080\" protocol : \"rabbitmq\" The ClusterStream reconciler will detect this attribute and converted it into status.binding.name in compliance with the ProvisioningService specification. ... status : binding : name : \"my-service-binding-ref\" ... Later is picked by the Service Binding Operator . It lookups and enforces all ServiceBinding resources with service name matching the provide binding name. For example apiVersion : servicebinding.io/v1beta1 kind : ServiceBinding metadata : name : streaming-runtime-rabbitmq spec : service : apiVersion : v1 kind : Secret name : my-service-binding-ref workload : apiVersion : apps/v1 kind : Deployment name : streaming-runtime-processor-possible-fraud-processor","title":"Overview"},{"location":"architecture/service-binding/service-binding/#enable-service-binding","text":"Install the Service Binding Operator . Any specification compliant operator can be used but we advice for the VMWare-Tanzu operator: kubectl apply -f https://github.com/vmware-tanzu/servicebinding/releases/download/v0.7.1/service-bindings-0.7.1.yaml Create Kubernetes Secrets for the protected services (e.g. Kafka, RabbitMQ \u2026). Note: When operators are used to provision those services, later create the needed secrets automatically. Follow the service operator instructions to find the names of the generated secrets. apiVersion : v1 kind : Secret metadata : name : streaming-runtime-rabbitmq-secret type : servicebinding.io/rabbitmq stringData : type : rabbitmq provider : rabbitmq host : rabbitmq.default.svc.cluster.local port : \"5672\" # demo credentials username : guest password : guest Create Service Binding contracts to bind the desired services and workloads. apiVersion : servicebinding.io/v1beta1 kind : ServiceBinding metadata : name : streaming-runtime-rabbitmq spec : service : apiVersion : v1 kind : Secret name : streaming-runtime-rabbitmq-secret workload : apiVersion : apps/v1 kind : Deployment name : streaming-runtime-processor-possible-fraud-processor env : - name : SPRING_RABBITMQ_PASSWORD key : password - name : SPRING_RABBITMQ_USERNAME key : username Add binding attribute to Stream resource to refer the service secret name apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : udf-output-possible-fraud-stream spec : name : udf-output-possible-fraud keys : [ \"card_id\" ] streamMode : [ \"write\" ] protocol : \"rabbitmq\" # Binding refers to a Secret with the same name. The stream controller uses this binding to configure ClusterStream's auto-creation binding : \"streaming-runtime-rabbitmq-secret\" storage : clusterStream : \"udf-output-possible-fraud-cluster-stream\" Future Work The Streaming Runtime should be able to create and manage the ServiceBinding objects internally","title":"Enable Service Binding"},{"location":"architecture/streams/overview/","text":"Streams The Streams CRD represents storage-at-rest of time-ordered attribute-partitioned data, such as a Kafka topic, or RabbitMQ Stream, exchange/queue. The Stream represents the binder/channel between two or more Processors . apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : {} spec : # Name of the Stream name : <string> # (Optional) Data Schema and Time Attributes dataSchemaContext : inline : Schema : <inline-schema> #(or schema : <meta-schema> ) ... timeAttributes : - name : ... watermark : ... # Attributes used as partitioning keys (either keys or keyExpression is allowed) keys : [ <string> ] # Attributes used as partitioning keys (either keys or keyExpression is allowed) keyExpression : [ <string> ] # Stream mode that the stream will be used for streamMode : [ <string> ] # Protocol to be used for the stream e.g. kafka protocol : <string> # (optional) Binding refers to a Secret with the same name. # The stream controller uses this binding to configure ClusterStream's auto-creation. binding : <string> storage : # Name of the ClusterStream resource clusterStream : <string> For a detailed description of attributes of the resource please read stream-crd.yaml The namespaced Streams declared (created) by a developer are backed by a ClusterStream resource which is controlled and provisioned by the administrator. ClusterStream relationship The ClusterStreams and the Streams follow the PersistentVolume model: namespaced Stream declared by a developer (ala PVC ) is backed by a ClusterStream resource (ala PV ) which is controlled and provisioned by the administrator. For convenience during the development stage, the SR operator auto-provisions the ClusterStreams for all Streams that don't have explicitly declared them. Data Schema The Stream CRDs can provide an elaborate Data Schema to define the structure, time and serialization details of the Stream messages. The data schema context comprises a schema of the message payload along with additional time-attributes , metadata mappings and configuration options. Data Partitioning For SRP and SCS processor types the key and keyExpression attributes are used to configure a data partitioning of the streamed data. Service Binding The Service Binding Specification provides a Kubernetes-wide specification for communicating service secrets to workloads in an automated way. The Stream spec.binding allow to refer existing service binding service (aka secrets). Data Policies WIP Usage Streams can be used as either input or output in the stream processing system; When deployed each Stream resource is represented by a messaging middleware topic/queue/exchange. Similarly the Processor resources are represented by executable message processing processors that adhere to the Streaming Runtime platform requirements.","title":"Overview"},{"location":"architecture/streams/overview/#streams","text":"The Streams CRD represents storage-at-rest of time-ordered attribute-partitioned data, such as a Kafka topic, or RabbitMQ Stream, exchange/queue. The Stream represents the binder/channel between two or more Processors . apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : {} spec : # Name of the Stream name : <string> # (Optional) Data Schema and Time Attributes dataSchemaContext : inline : Schema : <inline-schema> #(or schema : <meta-schema> ) ... timeAttributes : - name : ... watermark : ... # Attributes used as partitioning keys (either keys or keyExpression is allowed) keys : [ <string> ] # Attributes used as partitioning keys (either keys or keyExpression is allowed) keyExpression : [ <string> ] # Stream mode that the stream will be used for streamMode : [ <string> ] # Protocol to be used for the stream e.g. kafka protocol : <string> # (optional) Binding refers to a Secret with the same name. # The stream controller uses this binding to configure ClusterStream's auto-creation. binding : <string> storage : # Name of the ClusterStream resource clusterStream : <string> For a detailed description of attributes of the resource please read stream-crd.yaml The namespaced Streams declared (created) by a developer are backed by a ClusterStream resource which is controlled and provisioned by the administrator.","title":"Streams"},{"location":"architecture/streams/overview/#clusterstream-relationship","text":"The ClusterStreams and the Streams follow the PersistentVolume model: namespaced Stream declared by a developer (ala PVC ) is backed by a ClusterStream resource (ala PV ) which is controlled and provisioned by the administrator. For convenience during the development stage, the SR operator auto-provisions the ClusterStreams for all Streams that don't have explicitly declared them.","title":"ClusterStream relationship"},{"location":"architecture/streams/overview/#data-schema","text":"The Stream CRDs can provide an elaborate Data Schema to define the structure, time and serialization details of the Stream messages. The data schema context comprises a schema of the message payload along with additional time-attributes , metadata mappings and configuration options.","title":"Data Schema"},{"location":"architecture/streams/overview/#data-partitioning","text":"For SRP and SCS processor types the key and keyExpression attributes are used to configure a data partitioning of the streamed data.","title":"Data Partitioning"},{"location":"architecture/streams/overview/#service-binding","text":"The Service Binding Specification provides a Kubernetes-wide specification for communicating service secrets to workloads in an automated way. The Stream spec.binding allow to refer existing service binding service (aka secrets).","title":"Service Binding"},{"location":"architecture/streams/overview/#data-policies","text":"WIP","title":"Data Policies"},{"location":"architecture/streams/overview/#usage","text":"Streams can be used as either input or output in the stream processing system; When deployed each Stream resource is represented by a messaging middleware topic/queue/exchange. Similarly the Processor resources are represented by executable message processing processors that adhere to the Streaming Runtime platform requirements.","title":"Usage"},{"location":"architecture/streams/streaming-data-schema/","text":"Stream Data Schema The Stream CRDs can provide a data schema context to define the structure, time and serialization details of the messages they represent. The data schema context comprises a schema of the message payload along with additional time-attributes, metadata mappings and configuration options. The data schema describes the structure of the message payload. For a convenience, several, semantically equivalent, schema representations are supported. The time attributes augment can assign process or event time to schema in order to support streaming data processing. The metadata fields can extend the schema with additional, \u201csynthetic\u201d, fields that are extracted or computed from the message\u2019s metadata. The options allow specifying some configuration details, such as serialization, encoding formats, or properties passed directly through the backend sql aggregation engines. Following diagram illustrates the relationships between the Data Schema Context parts: Schema (Payload Data Model) The schema describes the structure of the message payload. It can be expressed as an Avro Schema, standard SQL /DDL or using the custom Stream MetaSchema (defined in the CRD and validated as OpenAPISchema). Additionally the payload schema can be retrieved from a remote schema registry such as Confluent Schema Registry using the Avro schema. Following snippets show the same data structure using the different schema representations. Meta-Schema Inline Apache Avro Inline SQL namespace : test.ns name : PlayEvents fields : - name : duration type : long optional : true - name : event_time type : long_timestamp-millis metadata : from : timestamp readonly : true watermark : \"`event_time`- INTERVAL '30' SECONDS\" { \"type\" : \"record\" , \"name\" : \"PlayEvents\" , \"namespace\" : \"test.ns\" , \"fields\" : [{ \"name\" : \"duration\" , \"type\" : [ \"null\" , \"long\" ] }] } CREATE TABLE PlayEvents ( ` duration ` BIGINT ) Note how the Meta-Schema representation extends the data schema content with metadata and time-attributes information. For the other representations this information is applied in the outer Schema Data Context sections (see the examples below). Time Attributes Streaming data processing can process data based on different notions of time. Processing time refers to the machine\u2019s system time that is executing the respective operation. Event time refers to the processing of streaming data based on timestamps that are attached to each row. The timestamps can encode when an event has happened. Time attributes can be part of every Stream\u2019s data schema. They are defined when creating the Stream CR . Once a time attribute is defined, it can be referenced as a field in Processor\u2019s queries and used in time-based operations. Event Time Event time allows a Processor query to produce results based on timestamps in every message, allowing for consistent results despite out-of-order or late events. It also ensures the replayability of the results of the streaming pipeline when reading messages from persistent storage (such as Kafka). All representations Meta-Schema only dataSchemaContext : inline : Schema : <inline-schema> #(or schema : <meta-schema> ) ... timeAttributes : - name : event_time watermark : \"`event_time` - INTERVAL '30' SECONDS\" dataSchemaContext : schema : namespace : \u2026 name : Songs fields : #... - name : event_time type : long_timestamp-millis watermark : \"`event_time` - INTERVAL '30' SECONDS\" The general syntax for defining Event Time field, assumes adding a timeAttributes entry only with name and watermark expression! (note without or empty watermark stands for Process Time field). You can mix Meta-Schema and general definitions. The general syntax precedes. Processing Time Processing time allows the streaming processing to produce results based on the time of the local machine. It is the simplest notion of time, but it will generate non-deterministic results. Processing time does not require timestamp extraction or watermark generation. There are two ways to define a processing time attribute. All representations Meta-Schema only dataSchemaContext : inline : Schema : <inline-schema> # (or schema : <meta-schema> ) ... timeAttributes : - name : my_proctime_field dataSchemaContext : schema : namespace : \u2026 name : Songs fields : #... - name : my_proctime_field type : proctime The general syntax for defining Process Time field, assumes adding a timeAttributes entry only with name but without watermark! You can mix Meta-Schema and general definitions. In case of conflict the general syntax precedes. Metadata Represents a special class of schema fields that are inferred or computed from the message\u2019s metadata. For example the message\u2019s timestamp or headers can be used as schema fields. All representations Meta-Schema only dataSchemaContext : inline : Schema : <inline-schema> # (or schema : <meta-schema> ) ... metadataFields : - name : event_time type : long_timestamp-millis metadata : from : timestamp readonly : true dataSchemaContext : schema : namespace : \u2026 name : Songs fields : #... - name : event_time type : long_timestamp-millis metadata : from : timestamp readonly : true Options Various serialization/deserialization configurations, remote system connection or implementation optimizations. The streaming runtime will try to hide as much as possible those details from the end user by inferring them from the Stream or ClusterStream statuses or assume some reasonable defaults for the common use cases. Yet the end user can use the Options section to further configure/optimize or override the defaults. Primary Key Primary key constraint is a hint for Streaming processing to leverage for optimizations. It indicates that a field or a set of fields of a data schema are unique and they do not contain null. Neither of the fields in a primary key can be nullable. dataSchemaContext : inline : Schema : <inline-schema> #(or schema : <meta-schema> ) \u2026 primaryKey : [ \"song_id\" , \"genre\" ] Schema Formats Description Meta-Schema Format Informal description of the Meta-Schema format. YAML Default Description namespace: <my.name.space> Schema namespace. name: <schema-name> Schema name unique in the namespace. fields.name: <field-name> Schema name unique in the namespace. fields.type: <field-type> Same as Avro primitive types plus one additional type: proctime . Later indicates that this is a Process time field. Also you can use the shortcut format <type>_<logicalType> to set both the type and the logical type attributes of the field. fields.logicalType: <filed-logical-type> Same as Avro\u2019s logicalTypes . fields.optional: <true or false> false If this field is optional in the schema. metadata.from: <message metadata> field.name Name of the massage metadata attributes to be used as value of the metadata field. This is dependent on the type of the input. For example for Kafka we can use the message timestamp as a metadata field. metadata.readonly: <true/false > true Whenever this metadata field can be overridden when sending a new message to the binder. watermark: <expression> true The watermark expression. It indicates that the field used in the expression is an Event-Time field. Inline- SQL Format Uses standard ANSI SQL CREATE TABLE statements to define the data schema. The schema definition may look like this: CREATE TABLE SongPlays ( ` song_id ` BIGINT NOT NULL , ` album ` STRING , ` genre ` STRING NOT NULL , ` duration ` BIGINT , ` event_time ` TIMESTAMP ( 3 ) NOT NULL ) Table name stands for Schema name. Table column names represent the schema field names. The table column types (standard SQL types) represent the field types. The \u201cNOT NULL\u201d type constraint indicates if the schema field is optional or not. The column types used as Time Attribute MUST use the TIMESTAMP(3) type. No metadata or time attributes fields are defined in this schema. Inline-Avro Format Follows the Avro 1.11.0 schema specification. Uses the JSON representation. { \"type\" : \"record\" , \"name\" : \"SongPlays\" , \"namespace\" : \"net.tzolov.poc.playsongs.avro\" , \"fields\" : [{ \"name\" : \"song_id\" , \"type\" : \"long\" }, { \"name\" : \"album\" , \"type\" : [ \"null\" , \"string\" ] }, { \"name\" : \"artist\" , \"type\" : [ \"null\" , \"string\" ] }, { \"name\" : \"name\" , \"type\" : [ \"null\" , \"string\" ] }, { \"name\" : \"genre\" , \"type\" : \"string\" }, { \"name\" : \"duration\" , \"type\" : [ \"null\" , \"long\" ] }, { \"name\" : \"event_time\" , \"type\" : { \"type\" : \"long\" , \"logicalType\" : \"timestamp-millis\" } }] } Note the same format is used for the schemas stored in Schema-Registries. (N.B. Streaming runtime converts internally all different representations into Avro schema). Examples Here is an example payload schema represented using the Meta-Schema, Inline-Avro, Inline- SQL representation. Meta-Schema dataSchemaContext : schema : namespace : net.tzolov.poc.playsongs.avro name : PlayEvents fields : - name : song_id type : long - name : duration type : long optional : true - name : event_time type : long_timestamp-millis metadata : from : timestamp readonly : true watermark : \"`event_time`- INTERVAL '30' SECONDS\" options : ddl.scan.startup.mode : earliest-offset Inline-Avro dataSchemaContext : inline : type : avro schema : | { \"type\" : \"record\", \"name\" : \"PlayEvents\", \"namespace\" : \"net.tzolov.poc.playsongs.avro\", \"fields\" : [ { \"name\" : \"song_id\", \"type\" : \"long\" }, { \"name\" : \"duration\", \"type\" : [ \"null\", \"long\" ] } ] } metadataFields : - name : event_time type : long_timestamp-millis metadata : from : timestamp readonly : true timeAttributes : - name : event_time watermark : \"`event_time` - INTERVAL '30' SECONDS\" options : ddl.scan.startup.mode : earliest-offset Note that the Schema Registry referenced schema are special case of the Inline-Avro: dataSchemaContext : inline : type : avro-confluent schema:<schema-registry-url>/\u200b\u200bsubjects/<topic>-value/versions/latest metadataFields : - name : event_time type : long_timestamp-millis metadata : from : timestamp readonly : true timeAttributes : - name : event_time watermark : \"`event_time` - INTERVAL '30' SECONDS\" options : ddl.scan.startup.mode : earliest-offset If only the type: avro-confluent attributed is provided, with empty schema value then the streaming runtime will default the schema value to: /\u200b\u200bsubjects/ -value/versions/latest, assuming that the schema registry URL is provided via the Options. Inline- SQL (using a plain ANSI compliant SQL ) dataSchemaContext : inline : type : sql schema : | CREATE TABLE PlayEvents ( `song_id` BIGINT NOT NULL, `duration` BIGINT ) metadataFields : - name : event_time type : long_timestamp-millis metadata : from : timestamp readonly : true timeAttributes : - name : event_time watermark : \"`event_time` - INTERVAL '30' SECONDS\" options : ddl.scan.startup.mode : earliest-offset The above three data schema representations are semantically identical and for each of them the Processor will generate exactly the same executable physical schema. In case of Apache Flink, the processor will generate the following CREATE TABLE DDL: CREATE TABLE PlayEvents ( ` song_id ` BIGINT NOT NULL , ` duration ` BIGINT , ` the_kafka_key ` STRING , ` event_time ` TIMESTAMP ( 3 ) NOT NULL METADATA FROM 'timestamp' VIRTUAL , WATERMARK FOR ` event_time ` AS ` event_time ` - INTERVAL '30' SECONDS ) WITH ( 'properties.bootstrap.servers' = 'kafka.default.svc.cluster.local:9092' , 'connector' = 'kafka' , 'value.format' = 'avro-confluent' , 'key.format' = 'raw' , 'value.fields-include' = 'EXCEPT_KEY' , 'topic' = 'kafka-stream-playevents' , 'value.avro-confluent.url' = 'http://s-registry.default.svc.cluster.local:8081' , 'key.fields' = 'the_kafka_key' ) Note how the metadata and the time attributes are mapped. Also most of the options (in the WITH sections) are inferred from the Stream\u2019s status server address and some defaults.","title":"Stream Data Schema"},{"location":"architecture/streams/streaming-data-schema/#stream-data-schema","text":"The Stream CRDs can provide a data schema context to define the structure, time and serialization details of the messages they represent. The data schema context comprises a schema of the message payload along with additional time-attributes, metadata mappings and configuration options. The data schema describes the structure of the message payload. For a convenience, several, semantically equivalent, schema representations are supported. The time attributes augment can assign process or event time to schema in order to support streaming data processing. The metadata fields can extend the schema with additional, \u201csynthetic\u201d, fields that are extracted or computed from the message\u2019s metadata. The options allow specifying some configuration details, such as serialization, encoding formats, or properties passed directly through the backend sql aggregation engines. Following diagram illustrates the relationships between the Data Schema Context parts:","title":"Stream Data Schema"},{"location":"architecture/streams/streaming-data-schema/#schema-payload-data-model","text":"The schema describes the structure of the message payload. It can be expressed as an Avro Schema, standard SQL /DDL or using the custom Stream MetaSchema (defined in the CRD and validated as OpenAPISchema). Additionally the payload schema can be retrieved from a remote schema registry such as Confluent Schema Registry using the Avro schema. Following snippets show the same data structure using the different schema representations. Meta-Schema Inline Apache Avro Inline SQL namespace : test.ns name : PlayEvents fields : - name : duration type : long optional : true - name : event_time type : long_timestamp-millis metadata : from : timestamp readonly : true watermark : \"`event_time`- INTERVAL '30' SECONDS\" { \"type\" : \"record\" , \"name\" : \"PlayEvents\" , \"namespace\" : \"test.ns\" , \"fields\" : [{ \"name\" : \"duration\" , \"type\" : [ \"null\" , \"long\" ] }] } CREATE TABLE PlayEvents ( ` duration ` BIGINT ) Note how the Meta-Schema representation extends the data schema content with metadata and time-attributes information. For the other representations this information is applied in the outer Schema Data Context sections (see the examples below).","title":"Schema (Payload Data Model)"},{"location":"architecture/streams/streaming-data-schema/#time-attributes","text":"Streaming data processing can process data based on different notions of time. Processing time refers to the machine\u2019s system time that is executing the respective operation. Event time refers to the processing of streaming data based on timestamps that are attached to each row. The timestamps can encode when an event has happened. Time attributes can be part of every Stream\u2019s data schema. They are defined when creating the Stream CR . Once a time attribute is defined, it can be referenced as a field in Processor\u2019s queries and used in time-based operations.","title":"Time Attributes"},{"location":"architecture/streams/streaming-data-schema/#event-time","text":"Event time allows a Processor query to produce results based on timestamps in every message, allowing for consistent results despite out-of-order or late events. It also ensures the replayability of the results of the streaming pipeline when reading messages from persistent storage (such as Kafka). All representations Meta-Schema only dataSchemaContext : inline : Schema : <inline-schema> #(or schema : <meta-schema> ) ... timeAttributes : - name : event_time watermark : \"`event_time` - INTERVAL '30' SECONDS\" dataSchemaContext : schema : namespace : \u2026 name : Songs fields : #... - name : event_time type : long_timestamp-millis watermark : \"`event_time` - INTERVAL '30' SECONDS\" The general syntax for defining Event Time field, assumes adding a timeAttributes entry only with name and watermark expression! (note without or empty watermark stands for Process Time field). You can mix Meta-Schema and general definitions. The general syntax precedes.","title":"Event Time"},{"location":"architecture/streams/streaming-data-schema/#processing-time","text":"Processing time allows the streaming processing to produce results based on the time of the local machine. It is the simplest notion of time, but it will generate non-deterministic results. Processing time does not require timestamp extraction or watermark generation. There are two ways to define a processing time attribute. All representations Meta-Schema only dataSchemaContext : inline : Schema : <inline-schema> # (or schema : <meta-schema> ) ... timeAttributes : - name : my_proctime_field dataSchemaContext : schema : namespace : \u2026 name : Songs fields : #... - name : my_proctime_field type : proctime The general syntax for defining Process Time field, assumes adding a timeAttributes entry only with name but without watermark! You can mix Meta-Schema and general definitions. In case of conflict the general syntax precedes.","title":"Processing Time"},{"location":"architecture/streams/streaming-data-schema/#metadata","text":"Represents a special class of schema fields that are inferred or computed from the message\u2019s metadata. For example the message\u2019s timestamp or headers can be used as schema fields. All representations Meta-Schema only dataSchemaContext : inline : Schema : <inline-schema> # (or schema : <meta-schema> ) ... metadataFields : - name : event_time type : long_timestamp-millis metadata : from : timestamp readonly : true dataSchemaContext : schema : namespace : \u2026 name : Songs fields : #... - name : event_time type : long_timestamp-millis metadata : from : timestamp readonly : true","title":"Metadata"},{"location":"architecture/streams/streaming-data-schema/#options","text":"Various serialization/deserialization configurations, remote system connection or implementation optimizations. The streaming runtime will try to hide as much as possible those details from the end user by inferring them from the Stream or ClusterStream statuses or assume some reasonable defaults for the common use cases. Yet the end user can use the Options section to further configure/optimize or override the defaults.","title":"Options"},{"location":"architecture/streams/streaming-data-schema/#primary-key","text":"Primary key constraint is a hint for Streaming processing to leverage for optimizations. It indicates that a field or a set of fields of a data schema are unique and they do not contain null. Neither of the fields in a primary key can be nullable. dataSchemaContext : inline : Schema : <inline-schema> #(or schema : <meta-schema> ) \u2026 primaryKey : [ \"song_id\" , \"genre\" ]","title":"Primary Key"},{"location":"architecture/streams/streaming-data-schema/#schema-formats-description","text":"","title":"Schema Formats Description"},{"location":"architecture/streams/streaming-data-schema/#meta-schema-format","text":"Informal description of the Meta-Schema format. YAML Default Description namespace: <my.name.space> Schema namespace. name: <schema-name> Schema name unique in the namespace. fields.name: <field-name> Schema name unique in the namespace. fields.type: <field-type> Same as Avro primitive types plus one additional type: proctime . Later indicates that this is a Process time field. Also you can use the shortcut format <type>_<logicalType> to set both the type and the logical type attributes of the field. fields.logicalType: <filed-logical-type> Same as Avro\u2019s logicalTypes . fields.optional: <true or false> false If this field is optional in the schema. metadata.from: <message metadata> field.name Name of the massage metadata attributes to be used as value of the metadata field. This is dependent on the type of the input. For example for Kafka we can use the message timestamp as a metadata field. metadata.readonly: <true/false > true Whenever this metadata field can be overridden when sending a new message to the binder. watermark: <expression> true The watermark expression. It indicates that the field used in the expression is an Event-Time field.","title":"Meta-Schema Format"},{"location":"architecture/streams/streaming-data-schema/#inline-sql-format","text":"Uses standard ANSI SQL CREATE TABLE statements to define the data schema. The schema definition may look like this: CREATE TABLE SongPlays ( ` song_id ` BIGINT NOT NULL , ` album ` STRING , ` genre ` STRING NOT NULL , ` duration ` BIGINT , ` event_time ` TIMESTAMP ( 3 ) NOT NULL ) Table name stands for Schema name. Table column names represent the schema field names. The table column types (standard SQL types) represent the field types. The \u201cNOT NULL\u201d type constraint indicates if the schema field is optional or not. The column types used as Time Attribute MUST use the TIMESTAMP(3) type. No metadata or time attributes fields are defined in this schema.","title":"Inline-SQL Format"},{"location":"architecture/streams/streaming-data-schema/#inline-avro-format","text":"Follows the Avro 1.11.0 schema specification. Uses the JSON representation. { \"type\" : \"record\" , \"name\" : \"SongPlays\" , \"namespace\" : \"net.tzolov.poc.playsongs.avro\" , \"fields\" : [{ \"name\" : \"song_id\" , \"type\" : \"long\" }, { \"name\" : \"album\" , \"type\" : [ \"null\" , \"string\" ] }, { \"name\" : \"artist\" , \"type\" : [ \"null\" , \"string\" ] }, { \"name\" : \"name\" , \"type\" : [ \"null\" , \"string\" ] }, { \"name\" : \"genre\" , \"type\" : \"string\" }, { \"name\" : \"duration\" , \"type\" : [ \"null\" , \"long\" ] }, { \"name\" : \"event_time\" , \"type\" : { \"type\" : \"long\" , \"logicalType\" : \"timestamp-millis\" } }] } Note the same format is used for the schemas stored in Schema-Registries. (N.B. Streaming runtime converts internally all different representations into Avro schema).","title":"Inline-Avro Format"},{"location":"architecture/streams/streaming-data-schema/#examples","text":"Here is an example payload schema represented using the Meta-Schema, Inline-Avro, Inline- SQL representation.","title":"Examples"},{"location":"architecture/streams/streaming-data-schema/#meta-schema","text":"dataSchemaContext : schema : namespace : net.tzolov.poc.playsongs.avro name : PlayEvents fields : - name : song_id type : long - name : duration type : long optional : true - name : event_time type : long_timestamp-millis metadata : from : timestamp readonly : true watermark : \"`event_time`- INTERVAL '30' SECONDS\" options : ddl.scan.startup.mode : earliest-offset","title":"Meta-Schema"},{"location":"architecture/streams/streaming-data-schema/#inline-avro","text":"dataSchemaContext : inline : type : avro schema : | { \"type\" : \"record\", \"name\" : \"PlayEvents\", \"namespace\" : \"net.tzolov.poc.playsongs.avro\", \"fields\" : [ { \"name\" : \"song_id\", \"type\" : \"long\" }, { \"name\" : \"duration\", \"type\" : [ \"null\", \"long\" ] } ] } metadataFields : - name : event_time type : long_timestamp-millis metadata : from : timestamp readonly : true timeAttributes : - name : event_time watermark : \"`event_time` - INTERVAL '30' SECONDS\" options : ddl.scan.startup.mode : earliest-offset Note that the Schema Registry referenced schema are special case of the Inline-Avro: dataSchemaContext : inline : type : avro-confluent schema:<schema-registry-url>/\u200b\u200bsubjects/<topic>-value/versions/latest metadataFields : - name : event_time type : long_timestamp-millis metadata : from : timestamp readonly : true timeAttributes : - name : event_time watermark : \"`event_time` - INTERVAL '30' SECONDS\" options : ddl.scan.startup.mode : earliest-offset If only the type: avro-confluent attributed is provided, with empty schema value then the streaming runtime will default the schema value to: /\u200b\u200bsubjects/ -value/versions/latest, assuming that the schema registry URL is provided via the Options.","title":"Inline-Avro"},{"location":"architecture/streams/streaming-data-schema/#inline-sql","text":"(using a plain ANSI compliant SQL ) dataSchemaContext : inline : type : sql schema : | CREATE TABLE PlayEvents ( `song_id` BIGINT NOT NULL, `duration` BIGINT ) metadataFields : - name : event_time type : long_timestamp-millis metadata : from : timestamp readonly : true timeAttributes : - name : event_time watermark : \"`event_time` - INTERVAL '30' SECONDS\" options : ddl.scan.startup.mode : earliest-offset The above three data schema representations are semantically identical and for each of them the Processor will generate exactly the same executable physical schema. In case of Apache Flink, the processor will generate the following CREATE TABLE DDL: CREATE TABLE PlayEvents ( ` song_id ` BIGINT NOT NULL , ` duration ` BIGINT , ` the_kafka_key ` STRING , ` event_time ` TIMESTAMP ( 3 ) NOT NULL METADATA FROM 'timestamp' VIRTUAL , WATERMARK FOR ` event_time ` AS ` event_time ` - INTERVAL '30' SECONDS ) WITH ( 'properties.bootstrap.servers' = 'kafka.default.svc.cluster.local:9092' , 'connector' = 'kafka' , 'value.format' = 'avro-confluent' , 'key.format' = 'raw' , 'value.fields-include' = 'EXCEPT_KEY' , 'topic' = 'kafka-stream-playevents' , 'value.avro-confluent.url' = 'http://s-registry.default.svc.cluster.local:8081' , 'key.fields' = 'the_kafka_key' ) Note how the metadata and the time attributes are mapped. Also most of the options (in the WITH sections) are inferred from the Stream\u2019s status server address and some defaults.","title":"Inline-SQL"},{"location":"samples/instructions/","text":"All use-cases are organized in folders named of after the use-case, each containing two files: streaming-runtime-samples/ <use-case-folder>/ streaming-pipeline.yaml data-generator.yaml streaming-pipeline.yaml is a manifest of Streaming-Runtime custom resources, such as ClusterStream , Stream and Processor , defining the use-case data processing pipeline. data-generator.yaml manifest deploys the Data Generator that continuously generates realistic test data for this particular use case. Run a Sample Follow the Streaming Runtime installation instructions to deploy the operator. Next from within the streaming-runtime-samples directory, deploy the use-case streaming pipeline: kubectl apply -f '<use-case-folder>/streaming-pipeline.yaml' -n streaming-runtime and the data generator to provide test data for this use case: kubectl apply -f '<use-case-folder>/data-generator.yaml' -n streaming-runtime Note Substitute the <use-case-folder> placeholder with the folder name of the use-case of choice. Explore the Results All input and output streams are backed by messaging systems such as Apache Kafka or RabbitMQ and we can explore the messages exchanged through the pipeline. Explore Apache Kafka Topics Using Kowl UI The auto-provisioned Apache Kafka clusters come pre-configured with the Apache Kowl UI visualization tool. To access it you need to forward the 80 port first: kubectl port-forward svc/kafka-kowl-ui 8082 :80 -n streaming-runtime Then open http://localhost:8082/topics or http://localhost:8082/schema-registry Tpics Tpics Details Schema Registry Using Command Line Use the kubectl get all to find the Kafka broker pod name and then kubectl exec -it pod/<your-kafka-pod> -- /bin/bash ` to SSH to kafka broker container. From within the kafka-broker container use the bin utils to list the topics or check their content: /opt/kafka/bin/kafka-topics.sh --list --bootstrap-server localhost:9092 Then to list the topic content: /opt/kafka/bin/kafka-console-consumer.sh --topic <topic-name> --from-beginning --bootstrap-server localhost:9092 To delete a topic: /opt/kafka/bin/kafka-topics.sh --delete --topic <topic-name> --bootstrap-server localhost:9092 Rabbit Queues To access the Rabbit management UI first forward the 15672 port: kubectl port-forward svc/rabbitmq 15672 :15672 -n streaming-runtime Then open http://localhost:15672/#/exchanges and find the exchange name related to your use-case. Open the Queues tab and create new queue called myTempQueue (use the default configuration). Go back to the Exchang tab, select the use-case exchange and bind it to the new myTempQueue queue, with # as a Routing key ! From the Queue tab select the myTempQueue queue and click the Get Messages button.","title":"How to Run"},{"location":"samples/instructions/#run-a-sample","text":"Follow the Streaming Runtime installation instructions to deploy the operator. Next from within the streaming-runtime-samples directory, deploy the use-case streaming pipeline: kubectl apply -f '<use-case-folder>/streaming-pipeline.yaml' -n streaming-runtime and the data generator to provide test data for this use case: kubectl apply -f '<use-case-folder>/data-generator.yaml' -n streaming-runtime Note Substitute the <use-case-folder> placeholder with the folder name of the use-case of choice.","title":"Run a Sample"},{"location":"samples/instructions/#explore-the-results","text":"All input and output streams are backed by messaging systems such as Apache Kafka or RabbitMQ and we can explore the messages exchanged through the pipeline.","title":"Explore the Results"},{"location":"samples/instructions/#explore-apache-kafka-topics","text":"","title":"Explore Apache Kafka Topics"},{"location":"samples/instructions/#using-kowl-ui","text":"The auto-provisioned Apache Kafka clusters come pre-configured with the Apache Kowl UI visualization tool. To access it you need to forward the 80 port first: kubectl port-forward svc/kafka-kowl-ui 8082 :80 -n streaming-runtime Then open http://localhost:8082/topics or http://localhost:8082/schema-registry Tpics Tpics Details Schema Registry","title":"Using Kowl UI"},{"location":"samples/instructions/#using-command-line","text":"Use the kubectl get all to find the Kafka broker pod name and then kubectl exec -it pod/<your-kafka-pod> -- /bin/bash ` to SSH to kafka broker container. From within the kafka-broker container use the bin utils to list the topics or check their content: /opt/kafka/bin/kafka-topics.sh --list --bootstrap-server localhost:9092 Then to list the topic content: /opt/kafka/bin/kafka-console-consumer.sh --topic <topic-name> --from-beginning --bootstrap-server localhost:9092 To delete a topic: /opt/kafka/bin/kafka-topics.sh --delete --topic <topic-name> --bootstrap-server localhost:9092","title":"Using Command Line"},{"location":"samples/instructions/#rabbit-queues","text":"To access the Rabbit management UI first forward the 15672 port: kubectl port-forward svc/rabbitmq 15672 :15672 -n streaming-runtime Then open http://localhost:15672/#/exchanges and find the exchange name related to your use-case. Open the Queues tab and create new queue called myTempQueue (use the default configuration). Go back to the Exchang tab, select the use-case exchange and bind it to the new myTempQueue queue, with # as a Routing key ! From the Queue tab select the myTempQueue queue and click the Get Messages button.","title":"Rabbit Queues"},{"location":"samples/overview/","text":"Samples below demonstrate how to implement various streaming and event-driven use case scenarios with the help of the Streaming Runtime . The setup instructions helps to setup the demo infrastructure (e.g. minikube) and to explore the demo results - e.g. exploring the Apache Kafka topics and/or RabbitMQ queues data. Anomaly Detection ( FSQL , SRP )- detect, in real time, suspicious credit card transactions, and extract them for further processing. Clickstream Analysis ( FSQL , SRP ) - for an input clickstream stream, we want to know who are the high status customers, currently using the website so that we can engage with them or to find how much they buy or how long they stay on the site that day. IoT Monitoring analysis ( FSQL , SRP ) - real-time analysis of IoT monitoring log. Streaming Music Service ( FSQL , SRP ) - music ranking application that continuously computes the latest Top-K music charts based on song play events collected in real-time. Spring Cloud Stream pipeline ( SCS ) - show how to build streaming pipelines using Spring Cloud Stream application as processors. ... more to come","title":"Overview"},{"location":"samples/tutorials/","text":"Tutorials The step by step tutorials introduce the SR features and how to use them. Quick start Follow the Streaming Runtime Install instructions to instal the Streaming Runtime operator. Deploy a selected tutorial data pipeline: kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/tutorial/<select-tutorial-pipeline>.yaml' -n streaming-runtime Run random data generation sent to the input stream: kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/tutorials/data-generator.yaml' -n streaming-runtime Follow the explore results instructions to see what data is generated and how it is processed though the pipeline. To delete the data pipeline and the data generator: kubectl delete srs,srcs,srp --all -n streaming-runtime kubectl delete deployments,svc -l app = authorization-attempts-data-generator -n streaming-runtime 1. Message Retransmission Processor re-transmits, unchanged, the events/messages received from the input (data-in) Stream into the output (data-out) Stream. The data-in and data-out Stream ( CRD ) resources are auto-provisioned using the runtime operator defaults, such a Kafka as a default protocol. The Stream resources, in turn, auto-provision their ClusterStreams ( CRD ) applying the ' -cluster-stream' naming convention. Finally the ClusterStream controllers will provision the required brokers for the target protocols (e.g. Kafka, RabbitMQ...). Currently 3 processor types are supported (if omitted is defaults to SRP ): SRP (default) - time-windowed, side-car UDF processor. SCS - Spring Cloud Stream/Function processor. FSQL - Apache Flink (inline) streaming SQL processor. One can combine multiple different processor types in the same data pipelines. 1. Message Retransmission # 1. Message Retransmission apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : re-transmission-processor spec : type : SRP inputs : - name : data-in outputs : - name : data-out 2. Multibinder Bridge Configure the Stream resources explicitly. The spe.protocol instructs what broker to be provisioned for this Stream. Current runtime implementation is using only Apache Kafka and RabbitMQ, but it can easily extended to support those additional Binders (e.g. message stream brokers): RabbitMQ Apache Kafka Amazon Kinesis Google PubSub Solace PubSub+ Azure Event Hubs Azure Service Bus Queue Binder Azure Service Bus Topic Binder Apache RocketMQ The Stream resource represent a Binder-Access-Request to the ClusterStream resource that provisions it. When the referred ClusterStream resource is not defined the Stream reconcile Controller will try to auto-provision a ClusterStreams (unless this behavior is disabled). It is the ClusterStream that provisions the required Binders for the target protocols (e.g. Kafka, RabbitMQ...). Different protocol deployment options are available. It defaults to built-in protocol adapters but can be configured to use operators such as RabbitOperator, Strimzi or alike instead! 2. Multibinder Bridge # 2. Multibinder (e.g. multi-message brokers) Bridge apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : data-in-stream spec : name : data-in protocol : \"kafka\" --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : multibinder-processor spec : type : SRP inputs : - name : data-in-stream outputs : - name : data-out-stream --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : data-out-stream spec : name : data-out protocol : \"rabbitmq\" # attributes: # protocolAdapterName: \"rabbitmq-operator\" # # Prerequisites to provision RabbitMQ clusters with the the \"rabbitmq-operator\": # https://vmware-tanzu.github.io/streaming-runtimes/install/#optional-install-rabbitmq-cluster-and-message-topology-operators # 2.1 Multibinder Bridge - production env In production environment the Streaming Runtime will not be allowed to auto-provision the messaging brokers dynamically. Instead the Administrator will provision the required messaging middleware and declare ClusterStream to provide managed and controlled access to it. The ClusterStreams and the Streams follow the PersistentVolume model: namespaced Stream declared by a developer (ala PVC) is backed by a ClusterStream resource (ala PV) which is controlled and provisioned by the administrator. 2.1 Multibinder Bridge - production env # 2.1 Multibinder Bridge - production env ################################################# # ADMIN responsibility ################################################# apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : ClusterStream metadata : name : kafka-cluster-stream spec : name : data-in streamModes : [ \"read\" , \"write\" ] # Note enforced yet storage : server : url : \"kafka.default.svc.cluster.local:9092\" protocol : \"kafka\" reclaimPolicy : \"Retain\" --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : ClusterStream metadata : name : rabbitmq-cluster-stream spec : name : data-out streamModes : [ \"read\" , \"write\" ] storage : server : url : \"rabbitmq.default.svc.cluster.local:5672\" protocol : \"rabbitmq\" reclaimPolicy : \"Retain\" --- ################################################# # DEVELOPER responsibility ################################################# apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : data-in-stream spec : name : data-in protocol : \"kafka\" storage : clusterStream : kafka-cluster-stream # Claims the pre-provisioned Kafka ClusterStream. --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : multibinder-processor spec : type : SRP inputs : - name : data-in-stream outputs : - name : data-out-stream --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : data-out-stream spec : name : data-out protocol : \"rabbitmq\" storage : clusterStream : rabbitmq-cluster-stream # Claims the pre-provisioned rabbitmq ClusterStream. 3. Inline Data Transformation The SpEL expressions can be applied to transform the input payloads on the fly. The spel.expression is applied on the inbound message and the result is used as outbound payload. The output.headers expression extracts values from the inbound headers/payload and injects new key/value headers to the outbound messages: =<[payload.|header.]expression> Note: SRP specific only. 3. Inline (e.g. in SRP Processor) Data Transformation # 3. Inline (e.g. in SRP Processor) Data Transformation apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : inline-transformation spec : type : SRP inputs : - name : data-in outputs : - name : data-out attributes : srp.output.headers : \"user=payload.fullName\" srp.spel.expression : ' '' {\" '' + #jsonPath(payload, '' $.fullName '' ) + '' \":\" '' + #jsonPath(payload, '' $.email '' ) + '' \"} '' ' 3.1 Polyglot UDF Transformation The ( SRP ) Processor can be assigned with custom User Defined Function running in a sidecar container next to the processor in the same Pod. Processor calls the UDF either for every received message or in the case of temporal aggregation calls it once the aggregate is ready. The communication between the Processor and the custom UDF is performed over gRPC using well defined Protocol Buffer contract. Because the Protocol Buffers are language-neutral, this allows implementing the UDF in any language of choice! (e.g. polyglot UDF ). Detailed UDF documentation: https://vmware-tanzu.github.io/streaming-runtimes/architecture/processors/srp/udf-overview/ Note: The inline transformations can be applied on the outbound message (e.g. the UDF response) before it is sent. Note: SRP specific feature. 3.1 Polyglot UDF Transformation # 3.1 Polyglot UDF Transformation apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : udf-transformation spec : type : SRP inputs : - name : data-in outputs : - name : data-out attributes : srp.grpcPort : \"50051\" template : spec : containers : - name : uppercase-grpc-python # Runs GRPC server on port 50051 image : ghcr.io/vmware-tanzu/streaming-runtimes/udf-uppercase-python:0.1 3.2 SCS (Spring Cloud Stream) Transformation Any Spring Cloud Stream or Spring Cloud Function application can be run as Processor. Just build a container image for the application and run it as `spec.type: SCS `` Processor type. Spring Cloud DataFlow provides 60+ pre-built SCS / SCF applications that can be used Out-Of-The-Box Use the environment variables to configure the Spring application. 3.2 SCS (Spring Cloud Stream) Transformation # 3.2 SCS (Spring Cloud Stream) Transformation apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : time-source spec : type : SCS outputs : - name : timestamps-stream template : spec : containers : - name : scdf-time-source-kafka image : springcloudstream/time-source-kafka:3.2.0 env : - name : SPRING_CLOUD_STREAM_POLLER_FIXED-DELAY value : \"2000\" - name : TIME_DATE-FORMAT value : \"dd/MM/yyyy HH:mm:ss\" --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : log-sink spec : type : SCS inputs : - name : timestamps-stream template : spec : containers : - name : scdf-log-sink-kafka image : springcloudstream/log-sink-kafka:3.2.0 env : - name : LOG_EXPRESSION value : \"'My uppercase timestamp is: ' + payload\" 4. Stateless Replication By default the Processor controller deploys one instance for every Processor. The spec.replicas is used to set the desired number of processor instances. For not-partitioned input the runtime creates, stateless, Kubernetes Deployment Pods for every processor instance and configures round-robing message delivery policy. Every inbound message is deliver to ONLY one processor instance selected on round-robing principle. The order of the instances is not guarantied. In case of partitioned input or processor 'forceStatefulSet=true' attribute, the runtime operator creates StatefulSet Pods with strict guarantees about the ordering and uniqueness of these Pods. Unlike a Deployment, the StatefulSet maintains a sticky identity for each of their Pods. These pods are created from the same spec, but are not interchangeable: each has a persistent identifier that it maintains across any rescheduling. If you want to use storage volumes to provide persistence for your workload, or use Stream partitioning, then StatefulSet is default configuration. Although individual Pods in a StatefulSet are susceptible to failure, the persistent Pod identifiers make it easier to match existing volumes to the new Pods that replace any that have failed. 4. Stateless Replication # 4. Stateless Replication apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : stateless-replication spec : type : SRP # 3 instances replicas : 3 inputs : - name : data-in outputs : - name : data-out # attributes: # forceStatefulSet: \"true\" 5. Partition by Field with Stateful Replication Processor types: SRP , SCS Documentation: Data Partitioning On the Steam resource that represents the partitioned connection, use the spec.keyExpression to define the what header or payload field to use as a discriminator to partition the data in the steam. Additionally use the spec.partitionCount property to configure the number of partitions you would like the incoming data to be partitioned into. Those properties are used to instruct the upstream processor(s) to provision the data partitioning configuration while the downstream processors are configured for partitioned inputs (e.g. enforce instance ordering and stateful connections). If the downstream processor is scaled out (e.g. replications: N ), then the streaming runtime will ensure StatefulSet replication instead of Deployment / ReplicationSet . Additionally, for the processors consuming partitioned Stream, the SR configures Pod's Ordinal Index to be used as partition instance-index. Later ensures that event after Pod failure/restart the same partitions will be (re)assigned to it. 5. Partition by Field with Stateful Replication # 5. Partition by Field with Stateful Replication apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : data-in-stream spec : name : data-in protocol : kafka --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : team-partition-processor spec : type : SRP inputs : - name : data-in-stream outputs : - name : partitioned-by-team-stream --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : partitioned-by-team-stream spec : name : partitioned-by-team protocol : kafka keyExpression : \"payload.team\" partitionCount : 3 --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : team-scores-processor spec : type : SRP replicas : 3 inputs : - name : partitioned-by-team-stream outputs : - name : team-scores-stream attributes : srp.spel.expression : \"'Team:' + #jsonPath(payload, '$.team') + ', Score:' + #jsonPath(payload, '$.score')\" --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : team-scores-stream spec : name : team-scores protocol : kafka 5.1 Partition by Field using Header Keys Variation of the 5-partition-by-field-with-stateful-replication.yaml that uses message headers as partitioning keys. The 'spec.keys' value in the partitioned Stream must exist as a header name in the messages carried by that stream. Also the 'data-in-stream' and 'team-scores-stream' Stream definitions are dropped in favor of auto-provisioned defaults. 5.1 Partition by Field with Stateful Replication (Header Keys) # 5.1 Partition by Field using Header Keys apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : user-partition-processor spec : type : SRP inputs : - name : data-in outputs : - name : partitioned-by-team-stream attributes : # The header name used for partitioning must match the outbound stream's spec.keys names!!! srp.output.headers : \"team=payload.team\" --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : partitioned-by-team-stream spec : name : partitioned-by-team protocol : kafka # The 'team' is expected to be a inbound message header name!!! keys : [ \"team\" ] partitionCount : 3 --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : user-scores-processor spec : type : SRP replicas : 3 inputs : - name : partitioned-by-team-stream outputs : - name : team-scores attributes : srp.spel.expression : \"'Team:' + #jsonPath(payload, '$.team') + ', Score:' + #jsonPath(payload, '$.score')\" 5.2 Partition by Field - RabbitMQ version Same as (5.) but with RabbitMQ brokers instead for the partitioning section of the pipeline. 5.2 Partition by Field with Stateful Replication (RabbitMQ) # 5.2 Partition by Field - RabbitMQ version apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : data-in-stream spec : name : data-in protocol : kafka --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : user-partition-processor spec : type : SRP inputs : - name : data-in-stream outputs : - name : partitioned-by-team-stream --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : partitioned-by-team-stream spec : name : partitioned-by-team protocol : rabbitmq keyExpression : \"payload.team\" partitionCount : 3 --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : user-scores-processor spec : type : SRP replicas : 3 inputs : - name : partitioned-by-team-stream outputs : - name : team-scores-stream attributes : srp.spel.expression : ' '' Team: '' + #jsonPath(payload, '' $.team '' ) + '' , Score: '' + #jsonPath(payload, '' $.score '' )' --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : team-scores-stream spec : name : team-scores protocol : rabbitmq 6. Tumbling Time-Window Aggregation The ( SRP ) processor supports Tumbling Time-Window Aggregation. The 'srp.window' attribute defines the window interval. The processor collects inbound messages into time-window groups based on the event-time computed for every message. The event-time is computed from message's payload or header metadata. The inbound Stream 'spec.dataSchemaContext.timeAttributes' defines which payload field (or header attribute) to be used as an Event-Time. Furthermore the Watermark expression allows configuring out-of-orderness. When no event-time is configured the processor defaults to the less reliable process-time as event-time. Complete Tumbling Time-Window documentation 6. Tumbling Time-Window Aggregation # 6. Tumbling Time-Window Aggregation apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : data-in-stream spec : name : data-in protocol : kafka dataSchemaContext : # Here the schema is used only with descriptive purpose! It is not used to validate the # stream content. # Check the FSQL samples to find how to enforce Avro schemas and use schema registries. schema : namespace : sr.poc.online.gaming name : User fields : - name : id type : string - name : fullName type : string - name : team type : string - name : email type : string - name : score type : int - name : score_time type : long_timestamp-millis timeAttributes : # Data field to be used as an event-time. Generated watermark uses 2 sec. out-of-orderness. # Note: The Out-Of-Order events are not Late Events! The LateEvents are handled differently. - name : score_time watermark : \"`score_time` - INTERVAL '2' SECOND\" --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : user-scores-processor spec : type : SRP inputs : - name : data-in-stream outputs : - name : user-scores-stream attributes : srp.window : 5s # Tumbling Time Window of 5 seconds. srp.window.idle.timeout : 60s # Allow partial release of idle time-windows. srp.lateEventMode : SIDE_CHANNEL # Send late events a side-channel stream. By default late events are discarded. template : spec : containers : - name : scores-by-user-javascript # The UDF implementation is in the './6-user-score-aggregation-js/aggregate.js' image : ghcr.io/vmware-tanzu/streaming-runtimes/user-score-js:latest --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : user-scores-stream spec : name : user-scores protocol : kafka ghcr.io/vmware-tanzu/streaming-runtimes/user-score-js:latest const udf = require ( 'streaming-runtime-udf-aggregator' ); // --------- UDF aggregation function -------- function aggregate ( headers , user , results ) { if ( ! results . has ( user . fullName )) { // Add new empty user aggregate to the result map results . set ( user . fullName , { from : headers . windowStartTime , to : headers . windowEndTime , name : user . fullName , totalScore : 0 , }); } // Increment user's score. let userAggregate = results . get ( user . fullName ); userAggregate . totalScore = Number . parseInt ( userAggregate . totalScore ) + Number . parseInt ( user . score ); } new udf . Aggregator ( aggregate ). start (); 6.1 Partition by Field with replicated Time-Window aggregation Reliably scale out the time-window aggregation processors by ensuring inbound data partitioning on the same key. Complete documentation 6.1 Partition by Field with replicated Time-Window aggregation # 6.1 Partition by Field with replicated Time-Window aggregation apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : data-in-stream spec : name : data-in protocol : kafka dataSchemaContext : schema : namespace : sr.poc.online.gaming name : User fields : - name : id type : string - name : fullName type : string - name : team type : string - name : email type : string - name : score type : int - name : score_time type : long_timestamp-millis timeAttributes : # Data field to be used as an event-time. # Generated watermark uses 2 sec. out-of-orderness. - name : score_time watermark : \"`score_time` - INTERVAL '2' SECOND\" --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : team-partition-processor spec : type : SRP inputs : - name : data-in-stream outputs : - name : partitioned-by-team-stream --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : partitioned-by-team-stream spec : name : partitioned-by-team protocol : kafka # Partition by Team name keyExpression : \"payload.team\" # Break into 3 partition groups partitionCount : 3 # Note: The inbound message event-time is injected automatically by the upstream processor into the 'header.eventtime'. # The downstream processor looks it up automatically, no need to define it explicitly via the Stream 'timeAttributes'. --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : team-scores-processor spec : type : SRP # Process the scores in parallel - # one processor instance per partition. replicas : 3 inputs : - name : partitioned-by-team-stream outputs : - name : team-scores-stream attributes : srp.window : 5s srp.window.idle.timeout : 140s template : spec : containers : - name : scores-by-team-javascript image : ghcr.io/vmware-tanzu/streaming-runtimes/team-score-js:latest --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : team-scores-stream spec : name : team-scores protocol : kafka ghcr.io/vmware-tanzu/streaming-runtimes/team-score-js:latest const udf = require ( 'streaming-runtime-udf-aggregator' ); // --------- UDF aggregation function -------- function aggregate ( headers , user , results ) { if ( ! results . has ( user . team )) { // Add new empty team aggregate to the result map results . set ( user . team , { from : headers . windowStartTime , to : headers . windowEndTime , team : user . team , totalScore : 0 , }); } // Increment team's score. let team = results . get ( user . team ); team . totalScore = Number . parseInt ( team . totalScore ) + Number . parseInt ( user . score ); } new udf . Aggregator ( aggregate ). start (); 7. FSQL processor examples Anomaly Detection ( FSQL , SRP )- detect, in real time, suspicious credit card transactions, and extract them for further processing. Clickstream Analysis ( FSQL , SRP ) - for an input clickstream stream, we want to know who are the high status customers, currently using the website so that we can engage with them or to find how much they buy or how long they stay on the site that day. IoT Monitoring analysis ( FSQL , SRP ) - real-time analysis of IoT monitoring log. Streaming Music Service ( FSQL , SRP ) - music ranking application that continuously computes the latest Top-K music charts based on song play events collected in real-time. 8. Secretes Management with Service Binding spec We can use the RabbitMQ Cluster Operator to provision our RabbitMQ Cluster and the ServiceBinding Operator to share the RabbitMQ credentials with the processors that would need to connect to it. Prerequisites: Service Binding Operator: https://vmware-tanzu.github.io/streaming-runtimes/install/#optional-install-service-binding-operator RabbitMQ Cluster and Message Topology Operators: https://vmware-tanzu.github.io/streaming-runtimes/install/#optional-install-rabbitmq-cluster-and-message-topology-operators Complete service binding documentation 8. Secretes Management with Service Binding spec # 8. Secretes Management with Service Binding spec. apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : data-in-stream spec : name : data-in protocol : \"kafka\" --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : multibinder-processor spec : type : SRP inputs : - name : data-in-stream outputs : - name : data-out-stream --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : data-out-stream spec : name : data-out protocol : \"rabbitmq\" # Binding refs a Secret with same name. The stream controller uses this binding to configure ClusterStream's auto-creation binding : \"data-out-stream-cluster-stream-default-user\" attributes : # Prerequisites to provision RabbitMQ clusters with the the \"rabbitmq-operator\": # https://vmware-tanzu.github.io/streaming-runtimes/install/#optional-install-rabbitmq-cluster-and-message-topology-operators protocolAdapterName : \"rabbitmq-operator\" --- apiVersion : servicebinding.io/v1beta1 kind : ServiceBinding metadata : name : streaming-runtime-rabbitmq spec : service : apiVersion : v1 kind : Secret # This is the secret generated by the RabbitMQ Cluster operator. Convention is: # '<cluster-stream-name>-default-user' # When the ClusterStream is automatically generated from the Stream Operator, the name convention is: # '<stream-name>-cluster-stream-default-user' name : data-out-stream-cluster-stream-default-user workload : apiVersion : apps/v1 kind : Deployment # Currently, the convention expects that the workload name is: 'srp-<your-processor-name>'. # TODO: Explore generating the ServiceBindings resources in the StreamingRuntime Operator. name : srp-multibinder-processor env : # As we know that this processor uses Spring RabbitMQ configuration, here we pass in the expected conf. - name : SPRING_RABBITMQ_PASSWORD key : password - name : SPRING_RABBITMQ_USERNAME key : username","title":"Step by Step Tutorials"},{"location":"samples/tutorials/#tutorials","text":"The step by step tutorials introduce the SR features and how to use them.","title":"Tutorials"},{"location":"samples/tutorials/#quick-start","text":"Follow the Streaming Runtime Install instructions to instal the Streaming Runtime operator. Deploy a selected tutorial data pipeline: kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/tutorial/<select-tutorial-pipeline>.yaml' -n streaming-runtime Run random data generation sent to the input stream: kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/tutorials/data-generator.yaml' -n streaming-runtime Follow the explore results instructions to see what data is generated and how it is processed though the pipeline. To delete the data pipeline and the data generator: kubectl delete srs,srcs,srp --all -n streaming-runtime kubectl delete deployments,svc -l app = authorization-attempts-data-generator -n streaming-runtime","title":"Quick start"},{"location":"samples/tutorials/#1-message-retransmission","text":"Processor re-transmits, unchanged, the events/messages received from the input (data-in) Stream into the output (data-out) Stream. The data-in and data-out Stream ( CRD ) resources are auto-provisioned using the runtime operator defaults, such a Kafka as a default protocol. The Stream resources, in turn, auto-provision their ClusterStreams ( CRD ) applying the ' -cluster-stream' naming convention. Finally the ClusterStream controllers will provision the required brokers for the target protocols (e.g. Kafka, RabbitMQ...). Currently 3 processor types are supported (if omitted is defaults to SRP ): SRP (default) - time-windowed, side-car UDF processor. SCS - Spring Cloud Stream/Function processor. FSQL - Apache Flink (inline) streaming SQL processor. One can combine multiple different processor types in the same data pipelines. 1. Message Retransmission # 1. Message Retransmission apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : re-transmission-processor spec : type : SRP inputs : - name : data-in outputs : - name : data-out","title":"1. Message Retransmission"},{"location":"samples/tutorials/#2-multibinder-bridge","text":"Configure the Stream resources explicitly. The spe.protocol instructs what broker to be provisioned for this Stream. Current runtime implementation is using only Apache Kafka and RabbitMQ, but it can easily extended to support those additional Binders (e.g. message stream brokers): RabbitMQ Apache Kafka Amazon Kinesis Google PubSub Solace PubSub+ Azure Event Hubs Azure Service Bus Queue Binder Azure Service Bus Topic Binder Apache RocketMQ The Stream resource represent a Binder-Access-Request to the ClusterStream resource that provisions it. When the referred ClusterStream resource is not defined the Stream reconcile Controller will try to auto-provision a ClusterStreams (unless this behavior is disabled). It is the ClusterStream that provisions the required Binders for the target protocols (e.g. Kafka, RabbitMQ...). Different protocol deployment options are available. It defaults to built-in protocol adapters but can be configured to use operators such as RabbitOperator, Strimzi or alike instead! 2. Multibinder Bridge # 2. Multibinder (e.g. multi-message brokers) Bridge apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : data-in-stream spec : name : data-in protocol : \"kafka\" --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : multibinder-processor spec : type : SRP inputs : - name : data-in-stream outputs : - name : data-out-stream --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : data-out-stream spec : name : data-out protocol : \"rabbitmq\" # attributes: # protocolAdapterName: \"rabbitmq-operator\" # # Prerequisites to provision RabbitMQ clusters with the the \"rabbitmq-operator\": # https://vmware-tanzu.github.io/streaming-runtimes/install/#optional-install-rabbitmq-cluster-and-message-topology-operators #","title":"2. Multibinder Bridge"},{"location":"samples/tutorials/#21-multibinder-bridge-production-env","text":"In production environment the Streaming Runtime will not be allowed to auto-provision the messaging brokers dynamically. Instead the Administrator will provision the required messaging middleware and declare ClusterStream to provide managed and controlled access to it. The ClusterStreams and the Streams follow the PersistentVolume model: namespaced Stream declared by a developer (ala PVC) is backed by a ClusterStream resource (ala PV) which is controlled and provisioned by the administrator. 2.1 Multibinder Bridge - production env # 2.1 Multibinder Bridge - production env ################################################# # ADMIN responsibility ################################################# apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : ClusterStream metadata : name : kafka-cluster-stream spec : name : data-in streamModes : [ \"read\" , \"write\" ] # Note enforced yet storage : server : url : \"kafka.default.svc.cluster.local:9092\" protocol : \"kafka\" reclaimPolicy : \"Retain\" --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : ClusterStream metadata : name : rabbitmq-cluster-stream spec : name : data-out streamModes : [ \"read\" , \"write\" ] storage : server : url : \"rabbitmq.default.svc.cluster.local:5672\" protocol : \"rabbitmq\" reclaimPolicy : \"Retain\" --- ################################################# # DEVELOPER responsibility ################################################# apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : data-in-stream spec : name : data-in protocol : \"kafka\" storage : clusterStream : kafka-cluster-stream # Claims the pre-provisioned Kafka ClusterStream. --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : multibinder-processor spec : type : SRP inputs : - name : data-in-stream outputs : - name : data-out-stream --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : data-out-stream spec : name : data-out protocol : \"rabbitmq\" storage : clusterStream : rabbitmq-cluster-stream # Claims the pre-provisioned rabbitmq ClusterStream.","title":"2.1 Multibinder Bridge - production env"},{"location":"samples/tutorials/#3-inline-data-transformation","text":"The SpEL expressions can be applied to transform the input payloads on the fly. The spel.expression is applied on the inbound message and the result is used as outbound payload. The output.headers expression extracts values from the inbound headers/payload and injects new key/value headers to the outbound messages: =<[payload.|header.]expression> Note: SRP specific only. 3. Inline (e.g. in SRP Processor) Data Transformation # 3. Inline (e.g. in SRP Processor) Data Transformation apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : inline-transformation spec : type : SRP inputs : - name : data-in outputs : - name : data-out attributes : srp.output.headers : \"user=payload.fullName\" srp.spel.expression : ' '' {\" '' + #jsonPath(payload, '' $.fullName '' ) + '' \":\" '' + #jsonPath(payload, '' $.email '' ) + '' \"} '' '","title":"3. Inline Data Transformation"},{"location":"samples/tutorials/#31-polyglot-udf-transformation","text":"The ( SRP ) Processor can be assigned with custom User Defined Function running in a sidecar container next to the processor in the same Pod. Processor calls the UDF either for every received message or in the case of temporal aggregation calls it once the aggregate is ready. The communication between the Processor and the custom UDF is performed over gRPC using well defined Protocol Buffer contract. Because the Protocol Buffers are language-neutral, this allows implementing the UDF in any language of choice! (e.g. polyglot UDF ). Detailed UDF documentation: https://vmware-tanzu.github.io/streaming-runtimes/architecture/processors/srp/udf-overview/ Note: The inline transformations can be applied on the outbound message (e.g. the UDF response) before it is sent. Note: SRP specific feature. 3.1 Polyglot UDF Transformation # 3.1 Polyglot UDF Transformation apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : udf-transformation spec : type : SRP inputs : - name : data-in outputs : - name : data-out attributes : srp.grpcPort : \"50051\" template : spec : containers : - name : uppercase-grpc-python # Runs GRPC server on port 50051 image : ghcr.io/vmware-tanzu/streaming-runtimes/udf-uppercase-python:0.1","title":"3.1 Polyglot UDF Transformation"},{"location":"samples/tutorials/#32-scs-spring-cloud-stream-transformation","text":"Any Spring Cloud Stream or Spring Cloud Function application can be run as Processor. Just build a container image for the application and run it as `spec.type: SCS `` Processor type. Spring Cloud DataFlow provides 60+ pre-built SCS / SCF applications that can be used Out-Of-The-Box Use the environment variables to configure the Spring application. 3.2 SCS (Spring Cloud Stream) Transformation # 3.2 SCS (Spring Cloud Stream) Transformation apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : time-source spec : type : SCS outputs : - name : timestamps-stream template : spec : containers : - name : scdf-time-source-kafka image : springcloudstream/time-source-kafka:3.2.0 env : - name : SPRING_CLOUD_STREAM_POLLER_FIXED-DELAY value : \"2000\" - name : TIME_DATE-FORMAT value : \"dd/MM/yyyy HH:mm:ss\" --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : log-sink spec : type : SCS inputs : - name : timestamps-stream template : spec : containers : - name : scdf-log-sink-kafka image : springcloudstream/log-sink-kafka:3.2.0 env : - name : LOG_EXPRESSION value : \"'My uppercase timestamp is: ' + payload\"","title":"3.2  SCS (Spring Cloud Stream) Transformation"},{"location":"samples/tutorials/#4-stateless-replication","text":"By default the Processor controller deploys one instance for every Processor. The spec.replicas is used to set the desired number of processor instances. For not-partitioned input the runtime creates, stateless, Kubernetes Deployment Pods for every processor instance and configures round-robing message delivery policy. Every inbound message is deliver to ONLY one processor instance selected on round-robing principle. The order of the instances is not guarantied. In case of partitioned input or processor 'forceStatefulSet=true' attribute, the runtime operator creates StatefulSet Pods with strict guarantees about the ordering and uniqueness of these Pods. Unlike a Deployment, the StatefulSet maintains a sticky identity for each of their Pods. These pods are created from the same spec, but are not interchangeable: each has a persistent identifier that it maintains across any rescheduling. If you want to use storage volumes to provide persistence for your workload, or use Stream partitioning, then StatefulSet is default configuration. Although individual Pods in a StatefulSet are susceptible to failure, the persistent Pod identifiers make it easier to match existing volumes to the new Pods that replace any that have failed. 4. Stateless Replication # 4. Stateless Replication apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : stateless-replication spec : type : SRP # 3 instances replicas : 3 inputs : - name : data-in outputs : - name : data-out # attributes: # forceStatefulSet: \"true\"","title":"4. Stateless Replication"},{"location":"samples/tutorials/#5-partition-by-field-with-stateful-replication","text":"Processor types: SRP , SCS Documentation: Data Partitioning On the Steam resource that represents the partitioned connection, use the spec.keyExpression to define the what header or payload field to use as a discriminator to partition the data in the steam. Additionally use the spec.partitionCount property to configure the number of partitions you would like the incoming data to be partitioned into. Those properties are used to instruct the upstream processor(s) to provision the data partitioning configuration while the downstream processors are configured for partitioned inputs (e.g. enforce instance ordering and stateful connections). If the downstream processor is scaled out (e.g. replications: N ), then the streaming runtime will ensure StatefulSet replication instead of Deployment / ReplicationSet . Additionally, for the processors consuming partitioned Stream, the SR configures Pod's Ordinal Index to be used as partition instance-index. Later ensures that event after Pod failure/restart the same partitions will be (re)assigned to it. 5. Partition by Field with Stateful Replication # 5. Partition by Field with Stateful Replication apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : data-in-stream spec : name : data-in protocol : kafka --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : team-partition-processor spec : type : SRP inputs : - name : data-in-stream outputs : - name : partitioned-by-team-stream --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : partitioned-by-team-stream spec : name : partitioned-by-team protocol : kafka keyExpression : \"payload.team\" partitionCount : 3 --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : team-scores-processor spec : type : SRP replicas : 3 inputs : - name : partitioned-by-team-stream outputs : - name : team-scores-stream attributes : srp.spel.expression : \"'Team:' + #jsonPath(payload, '$.team') + ', Score:' + #jsonPath(payload, '$.score')\" --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : team-scores-stream spec : name : team-scores protocol : kafka","title":"5. Partition by Field with Stateful Replication"},{"location":"samples/tutorials/#51-partition-by-field-using-header-keys","text":"Variation of the 5-partition-by-field-with-stateful-replication.yaml that uses message headers as partitioning keys. The 'spec.keys' value in the partitioned Stream must exist as a header name in the messages carried by that stream. Also the 'data-in-stream' and 'team-scores-stream' Stream definitions are dropped in favor of auto-provisioned defaults. 5.1 Partition by Field with Stateful Replication (Header Keys) # 5.1 Partition by Field using Header Keys apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : user-partition-processor spec : type : SRP inputs : - name : data-in outputs : - name : partitioned-by-team-stream attributes : # The header name used for partitioning must match the outbound stream's spec.keys names!!! srp.output.headers : \"team=payload.team\" --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : partitioned-by-team-stream spec : name : partitioned-by-team protocol : kafka # The 'team' is expected to be a inbound message header name!!! keys : [ \"team\" ] partitionCount : 3 --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : user-scores-processor spec : type : SRP replicas : 3 inputs : - name : partitioned-by-team-stream outputs : - name : team-scores attributes : srp.spel.expression : \"'Team:' + #jsonPath(payload, '$.team') + ', Score:' + #jsonPath(payload, '$.score')\"","title":"5.1 Partition by Field using Header Keys"},{"location":"samples/tutorials/#52-partition-by-field-rabbitmq-version","text":"Same as (5.) but with RabbitMQ brokers instead for the partitioning section of the pipeline. 5.2 Partition by Field with Stateful Replication (RabbitMQ) # 5.2 Partition by Field - RabbitMQ version apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : data-in-stream spec : name : data-in protocol : kafka --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : user-partition-processor spec : type : SRP inputs : - name : data-in-stream outputs : - name : partitioned-by-team-stream --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : partitioned-by-team-stream spec : name : partitioned-by-team protocol : rabbitmq keyExpression : \"payload.team\" partitionCount : 3 --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : user-scores-processor spec : type : SRP replicas : 3 inputs : - name : partitioned-by-team-stream outputs : - name : team-scores-stream attributes : srp.spel.expression : ' '' Team: '' + #jsonPath(payload, '' $.team '' ) + '' , Score: '' + #jsonPath(payload, '' $.score '' )' --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : team-scores-stream spec : name : team-scores protocol : rabbitmq","title":"5.2 Partition by Field - RabbitMQ version"},{"location":"samples/tutorials/#6-tumbling-time-window-aggregation","text":"The ( SRP ) processor supports Tumbling Time-Window Aggregation. The 'srp.window' attribute defines the window interval. The processor collects inbound messages into time-window groups based on the event-time computed for every message. The event-time is computed from message's payload or header metadata. The inbound Stream 'spec.dataSchemaContext.timeAttributes' defines which payload field (or header attribute) to be used as an Event-Time. Furthermore the Watermark expression allows configuring out-of-orderness. When no event-time is configured the processor defaults to the less reliable process-time as event-time. Complete Tumbling Time-Window documentation 6. Tumbling Time-Window Aggregation # 6. Tumbling Time-Window Aggregation apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : data-in-stream spec : name : data-in protocol : kafka dataSchemaContext : # Here the schema is used only with descriptive purpose! It is not used to validate the # stream content. # Check the FSQL samples to find how to enforce Avro schemas and use schema registries. schema : namespace : sr.poc.online.gaming name : User fields : - name : id type : string - name : fullName type : string - name : team type : string - name : email type : string - name : score type : int - name : score_time type : long_timestamp-millis timeAttributes : # Data field to be used as an event-time. Generated watermark uses 2 sec. out-of-orderness. # Note: The Out-Of-Order events are not Late Events! The LateEvents are handled differently. - name : score_time watermark : \"`score_time` - INTERVAL '2' SECOND\" --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : user-scores-processor spec : type : SRP inputs : - name : data-in-stream outputs : - name : user-scores-stream attributes : srp.window : 5s # Tumbling Time Window of 5 seconds. srp.window.idle.timeout : 60s # Allow partial release of idle time-windows. srp.lateEventMode : SIDE_CHANNEL # Send late events a side-channel stream. By default late events are discarded. template : spec : containers : - name : scores-by-user-javascript # The UDF implementation is in the './6-user-score-aggregation-js/aggregate.js' image : ghcr.io/vmware-tanzu/streaming-runtimes/user-score-js:latest --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : user-scores-stream spec : name : user-scores protocol : kafka ghcr.io/vmware-tanzu/streaming-runtimes/user-score-js:latest const udf = require ( 'streaming-runtime-udf-aggregator' ); // --------- UDF aggregation function -------- function aggregate ( headers , user , results ) { if ( ! results . has ( user . fullName )) { // Add new empty user aggregate to the result map results . set ( user . fullName , { from : headers . windowStartTime , to : headers . windowEndTime , name : user . fullName , totalScore : 0 , }); } // Increment user's score. let userAggregate = results . get ( user . fullName ); userAggregate . totalScore = Number . parseInt ( userAggregate . totalScore ) + Number . parseInt ( user . score ); } new udf . Aggregator ( aggregate ). start ();","title":"6. Tumbling Time-Window Aggregation"},{"location":"samples/tutorials/#61-partition-by-field-with-replicated-time-window-aggregation","text":"Reliably scale out the time-window aggregation processors by ensuring inbound data partitioning on the same key. Complete documentation 6.1 Partition by Field with replicated Time-Window aggregation # 6.1 Partition by Field with replicated Time-Window aggregation apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : data-in-stream spec : name : data-in protocol : kafka dataSchemaContext : schema : namespace : sr.poc.online.gaming name : User fields : - name : id type : string - name : fullName type : string - name : team type : string - name : email type : string - name : score type : int - name : score_time type : long_timestamp-millis timeAttributes : # Data field to be used as an event-time. # Generated watermark uses 2 sec. out-of-orderness. - name : score_time watermark : \"`score_time` - INTERVAL '2' SECOND\" --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : team-partition-processor spec : type : SRP inputs : - name : data-in-stream outputs : - name : partitioned-by-team-stream --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : partitioned-by-team-stream spec : name : partitioned-by-team protocol : kafka # Partition by Team name keyExpression : \"payload.team\" # Break into 3 partition groups partitionCount : 3 # Note: The inbound message event-time is injected automatically by the upstream processor into the 'header.eventtime'. # The downstream processor looks it up automatically, no need to define it explicitly via the Stream 'timeAttributes'. --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : team-scores-processor spec : type : SRP # Process the scores in parallel - # one processor instance per partition. replicas : 3 inputs : - name : partitioned-by-team-stream outputs : - name : team-scores-stream attributes : srp.window : 5s srp.window.idle.timeout : 140s template : spec : containers : - name : scores-by-team-javascript image : ghcr.io/vmware-tanzu/streaming-runtimes/team-score-js:latest --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : team-scores-stream spec : name : team-scores protocol : kafka ghcr.io/vmware-tanzu/streaming-runtimes/team-score-js:latest const udf = require ( 'streaming-runtime-udf-aggregator' ); // --------- UDF aggregation function -------- function aggregate ( headers , user , results ) { if ( ! results . has ( user . team )) { // Add new empty team aggregate to the result map results . set ( user . team , { from : headers . windowStartTime , to : headers . windowEndTime , team : user . team , totalScore : 0 , }); } // Increment team's score. let team = results . get ( user . team ); team . totalScore = Number . parseInt ( team . totalScore ) + Number . parseInt ( user . score ); } new udf . Aggregator ( aggregate ). start ();","title":"6.1 Partition by Field with replicated Time-Window aggregation"},{"location":"samples/tutorials/#7-fsql-processor-examples","text":"Anomaly Detection ( FSQL , SRP )- detect, in real time, suspicious credit card transactions, and extract them for further processing. Clickstream Analysis ( FSQL , SRP ) - for an input clickstream stream, we want to know who are the high status customers, currently using the website so that we can engage with them or to find how much they buy or how long they stay on the site that day. IoT Monitoring analysis ( FSQL , SRP ) - real-time analysis of IoT monitoring log. Streaming Music Service ( FSQL , SRP ) - music ranking application that continuously computes the latest Top-K music charts based on song play events collected in real-time.","title":"7. FSQL processor examples"},{"location":"samples/tutorials/#8-secretes-management-with-service-binding-spec","text":"We can use the RabbitMQ Cluster Operator to provision our RabbitMQ Cluster and the ServiceBinding Operator to share the RabbitMQ credentials with the processors that would need to connect to it. Prerequisites: Service Binding Operator: https://vmware-tanzu.github.io/streaming-runtimes/install/#optional-install-service-binding-operator RabbitMQ Cluster and Message Topology Operators: https://vmware-tanzu.github.io/streaming-runtimes/install/#optional-install-rabbitmq-cluster-and-message-topology-operators Complete service binding documentation 8. Secretes Management with Service Binding spec # 8. Secretes Management with Service Binding spec. apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : data-in-stream spec : name : data-in protocol : \"kafka\" --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : multibinder-processor spec : type : SRP inputs : - name : data-in-stream outputs : - name : data-out-stream --- apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : data-out-stream spec : name : data-out protocol : \"rabbitmq\" # Binding refs a Secret with same name. The stream controller uses this binding to configure ClusterStream's auto-creation binding : \"data-out-stream-cluster-stream-default-user\" attributes : # Prerequisites to provision RabbitMQ clusters with the the \"rabbitmq-operator\": # https://vmware-tanzu.github.io/streaming-runtimes/install/#optional-install-rabbitmq-cluster-and-message-topology-operators protocolAdapterName : \"rabbitmq-operator\" --- apiVersion : servicebinding.io/v1beta1 kind : ServiceBinding metadata : name : streaming-runtime-rabbitmq spec : service : apiVersion : v1 kind : Secret # This is the secret generated by the RabbitMQ Cluster operator. Convention is: # '<cluster-stream-name>-default-user' # When the ClusterStream is automatically generated from the Stream Operator, the name convention is: # '<stream-name>-cluster-stream-default-user' name : data-out-stream-cluster-stream-default-user workload : apiVersion : apps/v1 kind : Deployment # Currently, the convention expects that the workload name is: 'srp-<your-processor-name>'. # TODO: Explore generating the ServiceBindings resources in the StreamingRuntime Operator. name : srp-multibinder-processor env : # As we know that this processor uses Spring RabbitMQ configuration, here we pass in the expected conf. - name : SPRING_RABBITMQ_PASSWORD key : password - name : SPRING_RABBITMQ_USERNAME key : username","title":"8. Secretes Management with Service Binding spec"},{"location":"samples/anomaly-detection/anomaly-detection/","text":"Credit Card Anomaly Detection Imagine a stream of credit card authorization attempts, representing, for example, people swiping their chip cards into a reader or typing their number into a website. Such stream may look something like this: { \"card_number\" : \"1212-1221-1121-1234\" , \"card_type\" : \"discover\" , \"card_expiry\" : \"2013-9-12\" , \"name\" : \"Mr. Chester Stracke\" }, { \"card_number\" : \"1234-2121-1221-1211\" , \"card_type\" : \"dankort\" , \"card_expiry\" : \"2012-11-12\" , \"name\" : \"Preston Abbott\" } ... We would like to identify the suspicious transactions, in real time, and extract them for further investigations. For example we can count the incoming authorization attempts per card number and identify those authorizations that occurs suspiciously often. Lets use the Streams and Processors streaming-runtime resources to build such abnormal authorization detection application: The input stream, card-authorization , does not provide a time field for the time when the authorization attempt was performed. Such field would have been preferred option for the time widowing grouping. The next best thing is to use the message timestamp assigned by the message broker to each message. The implementation details section below explain how this is done to provision an additional event_time field to the authorization attempts data schema. The possible-fraud-detection processor leverages streaming SQL to compute the possible fraud attempts: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 INSERT INTO [[ STREAM : possible - fraud - stream ]] SELECT window_start , window_end , card_number , COUNT ( * ) AS authorization_attempts FROM TABLE ( TUMBLE ( TABLE [[ STREAM : card - authorizations - stream ]], DESCRIPTOR ( event_time ), INTERVAL '5' SECONDS ) ) GROUP BY window_start , window_end , card_number HAVING COUNT ( * ) > 5 Here we group the incoming authorization attempts by the card numbers ( lines: 12-13 ) and look only at those authorizations that have the same card number occurring suspiciously often ( 14-15 ). Then we put the suspicious card numbers into a new stream ( 1 ). But it would make no sense to count throughout the entire history of the authorization attempts! We are only interested in frequent authorization attempts that happen in short intervals of time. For this we split the incoming stream into a series of fixed-sized, non-overlapping and contiguous time intervals called Tumbling Windows ( 6-10 ). Here we aggregate the stream in intervals of 5 seconds assuming that 5 authorization attempts in 5 seconds would be hard for a person to do. Swiping the card or submitting the form five times within five seconds is a little weird. If we see that happening it is flagged as a possible fraud and inserted to the possible-fraud-stream ( 1 ). The possible-fraud-detection processor emits new possible-fraud stream containing the fraudulent transactions. Next with the help for the fraud-alert processor we can register a custom function UDF , that consumes the possible-fraud stream, investigates the suspicious transactions further for example to send alert emails. Following diagram visualizes the streaming-pipeline.yaml , implementing the use case with Stream and Processor resources: Quick start Follow the Streaming Runtime Install instructions to instal the operator. Install the anomaly detection streaming application: kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/anomaly-detection/streaming-pipeline.yaml' -n streaming-runtime Install the authorization attempts random data stream: kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/anomaly-detection/data-generator.yaml' -n streaming-runtime Follow the explore results instructions to see what data is generated and how it is processed though the pipeline. To delete the data pipeline and the data generator: kubectl delete srs,srcs,srp --all -n streaming-runtime kubectl delete deployments,svc -l app = authorization-attempts-data-generator -n streaming-runtime Implementation details We can implement the anomaly detection scenario with the help of the Streaming Runtime . We define three Streams and two Processor custom resources. Note: for the purpose of the demo we will skip the explicit CusterStream definitions and instead will enable auth-provisioning for those. Given that the input authorization attempts stream uses an Avro data format like this: { \"name\" : \"AuthorizationAttempts\" , \"namespace\" : \"com.tanzu.streaming.runtime.anomaly.detection\" , \"type\" : \"record\" , \"fields\" : [ { \"name\" : \"card_number\" , \"type\" : \"string\" }, { \"name\" : \"card_type\" , \"type\" : \"string\" }, { \"name\" : \"card_expiry\" , \"type\" : \"string\" }, { \"name\" : \"name\" , \"type\" : \"string\" } ] } We can represent it with the following custom Stream resource: apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : card-authorizations-stream spec : name : card-authorizations protocol : \"kafka\" storage : clusterStream : \"cluster-stream-card-authorizations\" streamMode : [ \"read\" ] keys : [ \"card_number\" ] dataSchemaContext : schema : namespace : com.tanzu.streaming.runtime.anomaly.detection name : AuthorizationAttempts fields : - name : card_number type : string - name : card_type type : string - name : card_expiry type : string - name : name type : string - name : event_time type : long_timestamp-millis metadata : from : timestamp readonly : true watermark : \"`event_time` - INTERVAL '3' SECONDS\" options : ddl.scan.startup.mode : earliest-offset The event_time field is auto-provisioned and assigned with Kafka message's timestamp. In addition, 3 seconds watermark is configured for the event_time field to tolerate out of order or late coming messages! The possible-fraud-detection Processor uses streaming SQL to compute the possible frauds: apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : possible-fraud-detection spec : type : FSQL inlineQuery : - \"INSERT INTO [[STREAM:possible-fraud-stream]] SELECT window_start, window_end, card_number, COUNT(*) AS authorization_attempts FROM TABLE(TUMBLE(TABLE [[STREAM:card-authorizations-stream]], DESCRIPTOR(event_time), INTERVAL '5' SECONDS)) GROUP BY window_start, window_end, card_number HAVING COUNT(*) > 5\" attributes : debugQuery : \"SELECT * FROM PossibleFraud\" debugExplain : \"2\" Processor outputs a new possible-fraud-stream Stream: apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : possible-fraud-stream spec : name : possible-fraud protocol : \"kafka\" storage : clusterStream : \"cluster-stream-possible-fraud\" streamMode : [ \"read\" , \"write\" ] keys : [ \"card_number\" ] dataSchemaContext : schema : namespace : com.tanzu.streaming.runtime.anomaly.detection name : PossibleFraud fields : - name : window_start type : long_timestamp-millis - name : window_end type : long_timestamp-millis - name : card_number type : string - name : authorization_attempts type : long options : ddl.key.fields : card_number ddl.value.format : \"json\" ddl.properties.allow.auto.create.topics : \"true\" ddl.scan.startup.mode : earliest-offset The possible-fraud-stream is given to fraud-alert processor configured with UDF to uppercase the payload content: apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : fraud-alert spec : type : SRP inputs : - name : \"possible-fraud-stream\" outputs : - name : \"fraud-alert-stream\" template : spec : containers : - name : possible-fraud-analysis-udf image : ghcr.io/vmware-tanzu/streaming-runtimes/udf-uppercase-go:0.1 Note that the UDF function can be implemented in any programming language. Finally, the output of the UDF function is send to the fraud-alert-stream stream defined like this: apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : fraud-alert-stream spec : name : fraud-alert keys : [ \"card_id\" ] streamMode : [ \"write\" ] protocol : \"rabbitmq\" storage : clusterStream : \"cluster-stream-fraud-alert-stream\" It uses RabbitMQ message broker and doesn't define an explicit schema assuming the payload data is just a byte-array.","title":"Anomaly Detection"},{"location":"samples/anomaly-detection/anomaly-detection/#credit-card-anomaly-detection","text":"Imagine a stream of credit card authorization attempts, representing, for example, people swiping their chip cards into a reader or typing their number into a website. Such stream may look something like this: { \"card_number\" : \"1212-1221-1121-1234\" , \"card_type\" : \"discover\" , \"card_expiry\" : \"2013-9-12\" , \"name\" : \"Mr. Chester Stracke\" }, { \"card_number\" : \"1234-2121-1221-1211\" , \"card_type\" : \"dankort\" , \"card_expiry\" : \"2012-11-12\" , \"name\" : \"Preston Abbott\" } ... We would like to identify the suspicious transactions, in real time, and extract them for further investigations. For example we can count the incoming authorization attempts per card number and identify those authorizations that occurs suspiciously often. Lets use the Streams and Processors streaming-runtime resources to build such abnormal authorization detection application: The input stream, card-authorization , does not provide a time field for the time when the authorization attempt was performed. Such field would have been preferred option for the time widowing grouping. The next best thing is to use the message timestamp assigned by the message broker to each message. The implementation details section below explain how this is done to provision an additional event_time field to the authorization attempts data schema. The possible-fraud-detection processor leverages streaming SQL to compute the possible fraud attempts: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 INSERT INTO [[ STREAM : possible - fraud - stream ]] SELECT window_start , window_end , card_number , COUNT ( * ) AS authorization_attempts FROM TABLE ( TUMBLE ( TABLE [[ STREAM : card - authorizations - stream ]], DESCRIPTOR ( event_time ), INTERVAL '5' SECONDS ) ) GROUP BY window_start , window_end , card_number HAVING COUNT ( * ) > 5 Here we group the incoming authorization attempts by the card numbers ( lines: 12-13 ) and look only at those authorizations that have the same card number occurring suspiciously often ( 14-15 ). Then we put the suspicious card numbers into a new stream ( 1 ). But it would make no sense to count throughout the entire history of the authorization attempts! We are only interested in frequent authorization attempts that happen in short intervals of time. For this we split the incoming stream into a series of fixed-sized, non-overlapping and contiguous time intervals called Tumbling Windows ( 6-10 ). Here we aggregate the stream in intervals of 5 seconds assuming that 5 authorization attempts in 5 seconds would be hard for a person to do. Swiping the card or submitting the form five times within five seconds is a little weird. If we see that happening it is flagged as a possible fraud and inserted to the possible-fraud-stream ( 1 ). The possible-fraud-detection processor emits new possible-fraud stream containing the fraudulent transactions. Next with the help for the fraud-alert processor we can register a custom function UDF , that consumes the possible-fraud stream, investigates the suspicious transactions further for example to send alert emails. Following diagram visualizes the streaming-pipeline.yaml , implementing the use case with Stream and Processor resources:","title":"Credit Card Anomaly Detection"},{"location":"samples/anomaly-detection/anomaly-detection/#quick-start","text":"Follow the Streaming Runtime Install instructions to instal the operator. Install the anomaly detection streaming application: kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/anomaly-detection/streaming-pipeline.yaml' -n streaming-runtime Install the authorization attempts random data stream: kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/anomaly-detection/data-generator.yaml' -n streaming-runtime Follow the explore results instructions to see what data is generated and how it is processed though the pipeline. To delete the data pipeline and the data generator: kubectl delete srs,srcs,srp --all -n streaming-runtime kubectl delete deployments,svc -l app = authorization-attempts-data-generator -n streaming-runtime","title":"Quick start"},{"location":"samples/anomaly-detection/anomaly-detection/#implementation-details","text":"We can implement the anomaly detection scenario with the help of the Streaming Runtime . We define three Streams and two Processor custom resources. Note: for the purpose of the demo we will skip the explicit CusterStream definitions and instead will enable auth-provisioning for those. Given that the input authorization attempts stream uses an Avro data format like this: { \"name\" : \"AuthorizationAttempts\" , \"namespace\" : \"com.tanzu.streaming.runtime.anomaly.detection\" , \"type\" : \"record\" , \"fields\" : [ { \"name\" : \"card_number\" , \"type\" : \"string\" }, { \"name\" : \"card_type\" , \"type\" : \"string\" }, { \"name\" : \"card_expiry\" , \"type\" : \"string\" }, { \"name\" : \"name\" , \"type\" : \"string\" } ] } We can represent it with the following custom Stream resource: apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : card-authorizations-stream spec : name : card-authorizations protocol : \"kafka\" storage : clusterStream : \"cluster-stream-card-authorizations\" streamMode : [ \"read\" ] keys : [ \"card_number\" ] dataSchemaContext : schema : namespace : com.tanzu.streaming.runtime.anomaly.detection name : AuthorizationAttempts fields : - name : card_number type : string - name : card_type type : string - name : card_expiry type : string - name : name type : string - name : event_time type : long_timestamp-millis metadata : from : timestamp readonly : true watermark : \"`event_time` - INTERVAL '3' SECONDS\" options : ddl.scan.startup.mode : earliest-offset The event_time field is auto-provisioned and assigned with Kafka message's timestamp. In addition, 3 seconds watermark is configured for the event_time field to tolerate out of order or late coming messages! The possible-fraud-detection Processor uses streaming SQL to compute the possible frauds: apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : possible-fraud-detection spec : type : FSQL inlineQuery : - \"INSERT INTO [[STREAM:possible-fraud-stream]] SELECT window_start, window_end, card_number, COUNT(*) AS authorization_attempts FROM TABLE(TUMBLE(TABLE [[STREAM:card-authorizations-stream]], DESCRIPTOR(event_time), INTERVAL '5' SECONDS)) GROUP BY window_start, window_end, card_number HAVING COUNT(*) > 5\" attributes : debugQuery : \"SELECT * FROM PossibleFraud\" debugExplain : \"2\" Processor outputs a new possible-fraud-stream Stream: apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : possible-fraud-stream spec : name : possible-fraud protocol : \"kafka\" storage : clusterStream : \"cluster-stream-possible-fraud\" streamMode : [ \"read\" , \"write\" ] keys : [ \"card_number\" ] dataSchemaContext : schema : namespace : com.tanzu.streaming.runtime.anomaly.detection name : PossibleFraud fields : - name : window_start type : long_timestamp-millis - name : window_end type : long_timestamp-millis - name : card_number type : string - name : authorization_attempts type : long options : ddl.key.fields : card_number ddl.value.format : \"json\" ddl.properties.allow.auto.create.topics : \"true\" ddl.scan.startup.mode : earliest-offset The possible-fraud-stream is given to fraud-alert processor configured with UDF to uppercase the payload content: apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : fraud-alert spec : type : SRP inputs : - name : \"possible-fraud-stream\" outputs : - name : \"fraud-alert-stream\" template : spec : containers : - name : possible-fraud-analysis-udf image : ghcr.io/vmware-tanzu/streaming-runtimes/udf-uppercase-go:0.1 Note that the UDF function can be implemented in any programming language. Finally, the output of the UDF function is send to the fraud-alert-stream stream defined like this: apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : fraud-alert-stream spec : name : fraud-alert keys : [ \"card_id\" ] streamMode : [ \"write\" ] protocol : \"rabbitmq\" storage : clusterStream : \"cluster-stream-fraud-alert-stream\" It uses RabbitMQ message broker and doesn't define an explicit schema assuming the payload data is just a byte-array.","title":"Implementation details"},{"location":"samples/clickstream/clickstream/","text":"Clickstream Analysis The Streaming ETL is a common place for many people to begin first when they start exploring the streaming data processing. With the streaming ETL we get some kind of data events coming into the streaming pipeline, and we want intelligent analysis to go out the other end. The clickstream analysis is common ETL data processing technique for helping understand the customer browsing behavior. Clickstream data is the pathway that a user takes through their online journey. For a single website it generally shows how the user progressed from search to purchase. The clickstream links together the actions a single user has taken within a single session. This means identifying where a search, click or purchase was performed within a single session. For example with clickstream analysis we can understand who are the high status customers currently using our websites, so that we can engage with them or find how much they buy or how long they stay on the site per day. Here is how the Stream and Processor resources can help us build a clickstream, data processing pipeline: The first input, user-stream , provides detailed information about the website registered users and looks like this: { \"user_id\" : \"407-41-3862\" , \"name\" : \"Olympia Koss\" , \"level\" : \"SILVER\" } { \"user_id\" : \"066-68-4140\" , \"name\" : \"Dr. Leah Daniel\" , \"level\" : \"GOLD\" } { \"user_id\" : \"722-61-1415\" , \"name\" : \"Steven Moore\" , \"level\" : \"GOLD\" } ... The second input, click-stream , streams the actions and paths the users take throughout their website browsing journey: { \"user_id\" : \"170-65-1094\" , \"page\" : 5535 , \"action\" : \"selection\" , \"device\" : \"computer\" , \"agent\" : \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36 OPR/43.0.2442.991\" } { \"user_id\" : \"804-31-3496\" , \"page\" : 30883 , \"action\" : \"checkout\" , \"device\" : \"tablet\" , \"agent\" : \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)\" } { \"user_id\" : \"011-54-8948\" , \"page\" : 18877 , \"action\" : \"products\" , \"device\" : \"mobile\" , \"agent\" : \"Mozilla/5.0 (iPhone; CPU iPhone OS 11_4_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/11.0 Mobile/15E148 Safari/604.1\" } ... The vip-join-and-filter Processor joins the input streams to enrich the click behavior with user details and filter in only the high status customer. Processor uses streaming SQL to continuously analyze the input streams and produces a new stream containing only enriched information for the Platinum level users: 1 . INSERT INTO VipActions 2 . SELECT 3 . Users . user_id , Users . name , Clicks . page , Clicks . action , Clicks . event_time 4 . FROM 5 . Clicks 6 . INNER JOIN 7 . Users ON Clicks . user_id = Users . user_id 8 . WHERE 9 . Users . level = 'PLATINUM' Computed VIP Actions are contentiously written the the output vip-action-stream . The second, vip-act-upon , Processor consumes the VIP-Actions events and allows us to implement a domain specific, User Defined Function that act and apply some business logic upon the VIP events. The UDF can be written in language of our choice! Following diagram visualizes the streaming-pipeline.yaml , implementing the clickstream application with the help of Stream and Processor resources: Quick start Follow the Streaming Runtime Install instructions to instal the operator. Install the Clickstream pipeline: kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/clickstream/streaming-pipeline.yaml' -n streaming-runtime Install the click-stream random data stream: kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/clickstream/data-generator.yaml' -n streaming-runtime Follow the explore results instructions to see what data is generated and how it is processed though the pipeline. Delete all pipelines: kubectl delete srs,srcs,srp --all -n streaming-runtime kubectl delete deployments,svc -l app = clickstream-data-generator -n streaming-runtime","title":"Clickstream Analysis"},{"location":"samples/clickstream/clickstream/#clickstream-analysis","text":"The Streaming ETL is a common place for many people to begin first when they start exploring the streaming data processing. With the streaming ETL we get some kind of data events coming into the streaming pipeline, and we want intelligent analysis to go out the other end. The clickstream analysis is common ETL data processing technique for helping understand the customer browsing behavior. Clickstream data is the pathway that a user takes through their online journey. For a single website it generally shows how the user progressed from search to purchase. The clickstream links together the actions a single user has taken within a single session. This means identifying where a search, click or purchase was performed within a single session. For example with clickstream analysis we can understand who are the high status customers currently using our websites, so that we can engage with them or find how much they buy or how long they stay on the site per day. Here is how the Stream and Processor resources can help us build a clickstream, data processing pipeline: The first input, user-stream , provides detailed information about the website registered users and looks like this: { \"user_id\" : \"407-41-3862\" , \"name\" : \"Olympia Koss\" , \"level\" : \"SILVER\" } { \"user_id\" : \"066-68-4140\" , \"name\" : \"Dr. Leah Daniel\" , \"level\" : \"GOLD\" } { \"user_id\" : \"722-61-1415\" , \"name\" : \"Steven Moore\" , \"level\" : \"GOLD\" } ... The second input, click-stream , streams the actions and paths the users take throughout their website browsing journey: { \"user_id\" : \"170-65-1094\" , \"page\" : 5535 , \"action\" : \"selection\" , \"device\" : \"computer\" , \"agent\" : \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36 OPR/43.0.2442.991\" } { \"user_id\" : \"804-31-3496\" , \"page\" : 30883 , \"action\" : \"checkout\" , \"device\" : \"tablet\" , \"agent\" : \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)\" } { \"user_id\" : \"011-54-8948\" , \"page\" : 18877 , \"action\" : \"products\" , \"device\" : \"mobile\" , \"agent\" : \"Mozilla/5.0 (iPhone; CPU iPhone OS 11_4_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/11.0 Mobile/15E148 Safari/604.1\" } ... The vip-join-and-filter Processor joins the input streams to enrich the click behavior with user details and filter in only the high status customer. Processor uses streaming SQL to continuously analyze the input streams and produces a new stream containing only enriched information for the Platinum level users: 1 . INSERT INTO VipActions 2 . SELECT 3 . Users . user_id , Users . name , Clicks . page , Clicks . action , Clicks . event_time 4 . FROM 5 . Clicks 6 . INNER JOIN 7 . Users ON Clicks . user_id = Users . user_id 8 . WHERE 9 . Users . level = 'PLATINUM' Computed VIP Actions are contentiously written the the output vip-action-stream . The second, vip-act-upon , Processor consumes the VIP-Actions events and allows us to implement a domain specific, User Defined Function that act and apply some business logic upon the VIP events. The UDF can be written in language of our choice! Following diagram visualizes the streaming-pipeline.yaml , implementing the clickstream application with the help of Stream and Processor resources:","title":"Clickstream Analysis"},{"location":"samples/clickstream/clickstream/#quick-start","text":"Follow the Streaming Runtime Install instructions to instal the operator. Install the Clickstream pipeline: kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/clickstream/streaming-pipeline.yaml' -n streaming-runtime Install the click-stream random data stream: kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/clickstream/data-generator.yaml' -n streaming-runtime Follow the explore results instructions to see what data is generated and how it is processed though the pipeline. Delete all pipelines: kubectl delete srs,srcs,srp --all -n streaming-runtime kubectl delete deployments,svc -l app = clickstream-data-generator -n streaming-runtime","title":"Quick start"},{"location":"samples/iot-monitoring/iot-monitoring/","text":"Real Time IoT Log Monitoring Imagine an IoT network, such as network of sensors, emitting monitoring events into a central service. We would like to analyze the incoming events for errors, and count and alert the most recent error types. Lets assume the input iot-monitoring-stream stream has a format like this: { \"error_code\" : \"C009_OUT_OF_RANGE\" , \"ts\" : 1645020042399 , \"type\" : \"ERROR\" , \"application\" : \"Hatity\" , \"version\" : \"1.16.4 \" , \"description\" : \"Chuck Norris can binary search unsorted data.\" } { \"error_code\" : \"C014_UNKNOWN\" , \"ts\" : 1645020042400 , \"type\" : \"DEBUG\" , \"application\" : \"Mat Lam Tam\" , \"version\" : \"5.0.9 \" , \"description\" : \"Chuck Norris doesn't bug hunt, as that signifies a probability of failure. He goes bug killing.\" } ... Then we can leverage the Stream and Processor resources to build an error analysis pipeline: The iot-monitoring-stream 's filed ts holds the time when the event was emitted. Additionally a watermark (of 3 sec.) is configured to handle out-of-order or late coming events! The sql-aggregator processor continuously filters in the erroneous events, groups them by type and counts them over a time-window intervals. We can express processor with a streaming SQL query like this: 1 . INSERT INTO [[ STREAM : error - count - stream ]] 2 . SELECT 3 . window_start , window_end , error_code , COUNT ( * ) AS error_count 4 . FROM 5 . TABLE ( 6 . TUMBLE ( 7 . TABLE [[ STREAM : iot - monitoring - stream ]], 8 . DESCRIPTOR ( ts ), 9 . INTERVAL '1' MINUTE 10 . ) 11 . ) 12 . WHERE type = 'ERROR' 13 . GROUP BY 14 . window_start , window_end , error_code Line ( 12 ) filters in only the error events. Those are grouped by error_code ( 12-13 ) to compute the counts of error events per error code. Finally a time windowing aggregation ( 6-10 ) is performed to compute the most recent 1 minute counts. The sql-aggregation processor in turn emits a stream of events, error-count-stream , like: { \"window_start\" : \"2022-02-16 14:18:00\" , \"window_end\" : \"2022-02-16 14:19:00\" , \"error_code\" : \"C007_INVALID_ARGUMENT\" , \"error_count\" : 16 } { \"window_start\" : \"2022-02-16 14:18:00\" , \"window_end\" : \"2022-02-16 14:19:00\" , \"error_code\" : \"C011_RESOURCE_EXHAUSTED\" , \"error_count\" : 28 } { \"window_start\" : \"2022-02-16 14:18:00\" , \"window_end\" : \"2022-02-16 14:19:00\" , \"error_code\" : \"C008_NOT_FOUND\" , \"error_count\" : 28 } The monitoring-utf processor registers a User Defined Function ( UDF ) to post-process the computed error-count-stream aggregates. For example the UDF can look for the root causes of the frequently occurring error or send alerting notifications to 3rd party systems. The UDF function can be implemented in any programming language as long as it adheres to the Streaming-Runtime gRPC protocol. Following diagram visualizes the streaming-pipeline.yaml , implementing the iot-monitoring application with the help of Stream and Processor resources: Quick start Follow the Streaming Runtime Install instructions to instal the operator. Install the IoT monitoring streaming application: kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/iot-monitoring/streaming-pipeline.yaml' -n streaming-runtime Deploy a random data stream generator: kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/iot-monitoring/data-generator.yaml' -n streaming-runtime Follow the explore results instructions to see what data is generated and how it is processed though the pipeline. Delete all pipelines: kubectl delete srs,srcs,srp --all -n streaming-runtime kubectl delete deployments,svc -l app = iot-monitoring-data-generator -n streaming-runtime Implementation details The above scenario is implemented with the help of the Streaming Runtime using three Streams and two Processor resources. (Note: for the purpose of the demo we skip the explicit CusterStream definitions and instead will enable auth-provisioning for those). Given that the input iot-monitoring-stream uses an Avro data format like this: namespace : com.tanzu.streaming.runtime.iot.log type : record name : MonitoringStream fields : - name : error_code type : string - name : ts type : type : long logicalType : timestamp-millis - name : type type : string - name : application type : string - name : version type : string - name : description type : string We can represent it with the following custom Stream resource with schema definition: apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : iot-monitoring-stream spec : protocol : \"kafka\" storage : clusterStream : \"iot-monitoring-cluster-stream\" streamMode : [ \"read\" ] keys : [ \"error_code\" ] dataSchemaContext : schema : namespace : com.tanzu.streaming.runtime.iot.log name : MonitoringStream fields : - name : error_code type : string - name : type type : string - name : application type : string - name : version type : string - name : description type : string - name : ts type : long_timestamp-millis watermark : \"`ts` - INTERVAL '3' SECONDS\" options : ddl.scan.startup.mode : earliest-offset The ts field is the timestamp when the event was emitted. We are adding also a 3 seconds watermark to tolerate out-of-order or late coming events! The input events aggregation Processor can be defined like this: apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : iot-monitoring-sql-aggregation spec : type : FSQL inlineQuery : - \"INSERT INTO [[STREAM:error-count-stream]] SELECT window_start, window_end, error_code, COUNT(*) AS error_count FROM TABLE(TUMBLE(TABLE [[STREAM:iot-monitoring-stream]], DESCRIPTOR(ts), INTERVAL '1' MINUTE)) WHERE type='ERROR' GROUP BY window_start, window_end, error_code\" It takes as input the iot-monitoring-stream and produces new error-count-stream stream populated with error counts aggregations, using JSON format: apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : error-count-stream spec : protocol : \"kafka\" storage : clusterStream : \"error-count-cluster-stream\" streamMode : [ \"read\" , \"write\" ] keys : [ \"error_code\" ] dataSchemaContext : schema : namespace : com.tanzu.streaming.runtime.iot.log name : ErrorCount fields : - name : window_start type : long_timestamp-millis - name : window_end type : long_timestamp-millis - name : error_code type : string - name : error_count type : long options : ddl.key.fields : error_code ddl.value.format : \"json\" ddl.properties.allow.auto.create.topics : \"true\" ddl.scan.startup.mode : earliest-offset Next the iot-monitoring-udf registers a custom ghcr.io/vmware-tanzu/streaming-runtimes/udf-uppercase-go:0.1 UDF that simply turns the payload to uppercase: apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : iot-monitoring-udf spec : type : SRP inputs : - name : \"error-count-stream\" outputs : - name : \"udf-output-error-count-stream\" template : spec : containers : - name : iot-monitoring-error-code-udf image : ghcr.io/vmware-tanzu/streaming-runtimes/udf-uppercase-go:0.1 Note that the UDF function can be implemented in any programming language. Finally, the output of the UDF function is send to the udf-output-error-count-stream stream defined like this: apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : udf-output-error-count-stream spec : keys : [ \"error_code\" ] streamMode : [ \"write\" ] protocol : \"rabbitmq\" storage : clusterStream : \"udf-output-error-count-cluster-stream\" It uses RabbitMQ message broker and doesn't define an explicit schema assuming the payload data is just a byte-array.","title":"IoT Monitoring"},{"location":"samples/iot-monitoring/iot-monitoring/#real-time-iot-log-monitoring","text":"Imagine an IoT network, such as network of sensors, emitting monitoring events into a central service. We would like to analyze the incoming events for errors, and count and alert the most recent error types. Lets assume the input iot-monitoring-stream stream has a format like this: { \"error_code\" : \"C009_OUT_OF_RANGE\" , \"ts\" : 1645020042399 , \"type\" : \"ERROR\" , \"application\" : \"Hatity\" , \"version\" : \"1.16.4 \" , \"description\" : \"Chuck Norris can binary search unsorted data.\" } { \"error_code\" : \"C014_UNKNOWN\" , \"ts\" : 1645020042400 , \"type\" : \"DEBUG\" , \"application\" : \"Mat Lam Tam\" , \"version\" : \"5.0.9 \" , \"description\" : \"Chuck Norris doesn't bug hunt, as that signifies a probability of failure. He goes bug killing.\" } ... Then we can leverage the Stream and Processor resources to build an error analysis pipeline: The iot-monitoring-stream 's filed ts holds the time when the event was emitted. Additionally a watermark (of 3 sec.) is configured to handle out-of-order or late coming events! The sql-aggregator processor continuously filters in the erroneous events, groups them by type and counts them over a time-window intervals. We can express processor with a streaming SQL query like this: 1 . INSERT INTO [[ STREAM : error - count - stream ]] 2 . SELECT 3 . window_start , window_end , error_code , COUNT ( * ) AS error_count 4 . FROM 5 . TABLE ( 6 . TUMBLE ( 7 . TABLE [[ STREAM : iot - monitoring - stream ]], 8 . DESCRIPTOR ( ts ), 9 . INTERVAL '1' MINUTE 10 . ) 11 . ) 12 . WHERE type = 'ERROR' 13 . GROUP BY 14 . window_start , window_end , error_code Line ( 12 ) filters in only the error events. Those are grouped by error_code ( 12-13 ) to compute the counts of error events per error code. Finally a time windowing aggregation ( 6-10 ) is performed to compute the most recent 1 minute counts. The sql-aggregation processor in turn emits a stream of events, error-count-stream , like: { \"window_start\" : \"2022-02-16 14:18:00\" , \"window_end\" : \"2022-02-16 14:19:00\" , \"error_code\" : \"C007_INVALID_ARGUMENT\" , \"error_count\" : 16 } { \"window_start\" : \"2022-02-16 14:18:00\" , \"window_end\" : \"2022-02-16 14:19:00\" , \"error_code\" : \"C011_RESOURCE_EXHAUSTED\" , \"error_count\" : 28 } { \"window_start\" : \"2022-02-16 14:18:00\" , \"window_end\" : \"2022-02-16 14:19:00\" , \"error_code\" : \"C008_NOT_FOUND\" , \"error_count\" : 28 } The monitoring-utf processor registers a User Defined Function ( UDF ) to post-process the computed error-count-stream aggregates. For example the UDF can look for the root causes of the frequently occurring error or send alerting notifications to 3rd party systems. The UDF function can be implemented in any programming language as long as it adheres to the Streaming-Runtime gRPC protocol. Following diagram visualizes the streaming-pipeline.yaml , implementing the iot-monitoring application with the help of Stream and Processor resources:","title":"Real Time IoT Log Monitoring"},{"location":"samples/iot-monitoring/iot-monitoring/#quick-start","text":"Follow the Streaming Runtime Install instructions to instal the operator. Install the IoT monitoring streaming application: kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/iot-monitoring/streaming-pipeline.yaml' -n streaming-runtime Deploy a random data stream generator: kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/iot-monitoring/data-generator.yaml' -n streaming-runtime Follow the explore results instructions to see what data is generated and how it is processed though the pipeline. Delete all pipelines: kubectl delete srs,srcs,srp --all -n streaming-runtime kubectl delete deployments,svc -l app = iot-monitoring-data-generator -n streaming-runtime","title":"Quick start"},{"location":"samples/iot-monitoring/iot-monitoring/#implementation-details","text":"The above scenario is implemented with the help of the Streaming Runtime using three Streams and two Processor resources. (Note: for the purpose of the demo we skip the explicit CusterStream definitions and instead will enable auth-provisioning for those). Given that the input iot-monitoring-stream uses an Avro data format like this: namespace : com.tanzu.streaming.runtime.iot.log type : record name : MonitoringStream fields : - name : error_code type : string - name : ts type : type : long logicalType : timestamp-millis - name : type type : string - name : application type : string - name : version type : string - name : description type : string We can represent it with the following custom Stream resource with schema definition: apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : iot-monitoring-stream spec : protocol : \"kafka\" storage : clusterStream : \"iot-monitoring-cluster-stream\" streamMode : [ \"read\" ] keys : [ \"error_code\" ] dataSchemaContext : schema : namespace : com.tanzu.streaming.runtime.iot.log name : MonitoringStream fields : - name : error_code type : string - name : type type : string - name : application type : string - name : version type : string - name : description type : string - name : ts type : long_timestamp-millis watermark : \"`ts` - INTERVAL '3' SECONDS\" options : ddl.scan.startup.mode : earliest-offset The ts field is the timestamp when the event was emitted. We are adding also a 3 seconds watermark to tolerate out-of-order or late coming events! The input events aggregation Processor can be defined like this: apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : iot-monitoring-sql-aggregation spec : type : FSQL inlineQuery : - \"INSERT INTO [[STREAM:error-count-stream]] SELECT window_start, window_end, error_code, COUNT(*) AS error_count FROM TABLE(TUMBLE(TABLE [[STREAM:iot-monitoring-stream]], DESCRIPTOR(ts), INTERVAL '1' MINUTE)) WHERE type='ERROR' GROUP BY window_start, window_end, error_code\" It takes as input the iot-monitoring-stream and produces new error-count-stream stream populated with error counts aggregations, using JSON format: apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : error-count-stream spec : protocol : \"kafka\" storage : clusterStream : \"error-count-cluster-stream\" streamMode : [ \"read\" , \"write\" ] keys : [ \"error_code\" ] dataSchemaContext : schema : namespace : com.tanzu.streaming.runtime.iot.log name : ErrorCount fields : - name : window_start type : long_timestamp-millis - name : window_end type : long_timestamp-millis - name : error_code type : string - name : error_count type : long options : ddl.key.fields : error_code ddl.value.format : \"json\" ddl.properties.allow.auto.create.topics : \"true\" ddl.scan.startup.mode : earliest-offset Next the iot-monitoring-udf registers a custom ghcr.io/vmware-tanzu/streaming-runtimes/udf-uppercase-go:0.1 UDF that simply turns the payload to uppercase: apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : iot-monitoring-udf spec : type : SRP inputs : - name : \"error-count-stream\" outputs : - name : \"udf-output-error-count-stream\" template : spec : containers : - name : iot-monitoring-error-code-udf image : ghcr.io/vmware-tanzu/streaming-runtimes/udf-uppercase-go:0.1 Note that the UDF function can be implemented in any programming language. Finally, the output of the UDF function is send to the udf-output-error-count-stream stream defined like this: apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : udf-output-error-count-stream spec : keys : [ \"error_code\" ] streamMode : [ \"write\" ] protocol : \"rabbitmq\" storage : clusterStream : \"udf-output-error-count-cluster-stream\" It uses RabbitMQ message broker and doesn't define an explicit schema assuming the payload data is just a byte-array.","title":"Implementation details"},{"location":"samples/spring-cloud-stream/tick-tock/","text":"Ticktock Pipeline The Spring Cloud Stream framework lets you easily build highly scalable event-driven microservices connected with shared messaging systems. Furthermore there is an extensive set (60+) of pre-built streaming applications that one can use out of the box. The Streaming-Runtime Processor resource provides seamless support for deploying Spring Cloud Stream application. Lest combine the time-source , transformer and log-sink streaming applications into a simple streaming pipeline: The time source processor generates timestamps at fixed, pre-configured intervals and emits them to the timestamps stream . The time.date-format property sets the desired date format. The transformer processor uses SpEL expressions to convert the input message payload and send the result to the output uppercase stream. Here the expression converts the payload to uppercase. The log sink processor listens for input messages from the uppercase stream and prints them to the standard console output. The log.expression property allows us to specify the format of the printed log messages. The Streams exchange plain text payloads (e.g. byte-array) and do not require dedicated schema definitions. The pre-built SCS streaming applications are build to support same message broker as input and output stream. For each application there are two builds one with Apache Kafka and another with RabbitMQ . It is still possible to re-compile those applications for different message brokers (e.g. protocols ) or even mixture for multiple protocols (e.g. multibinder ). The streaming-pipeline-tiktock.yaml puts the entire pipeline together and after deployed would look like: The streaming-pipeline-ticktock-partitioned-better.yaml shows how to data-partition the TickTock application leveraging the SCS Data-Partitioning capabilities. Quick start Follow the Streaming Runtime Install instructions to instal the operator. Install the anomaly detection streaming application: ticktock ticktock - partitioned kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/spring-cloud-stream/streaming-pipeline-ticktock.yaml' -n streaming-runtime kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/spring-cloud-stream/streaming-pipeline-ticktock-partitioned-better.yaml' -n streaming-runtime Follow the explore results instructions to see what data is generated and how it is processed though the pipeline. To delete the data pipeline and the data generator: kubectl delete srs,srcs,srp --all -n streaming-runtime","title":"SCS TickTock"},{"location":"samples/spring-cloud-stream/tick-tock/#ticktock-pipeline","text":"The Spring Cloud Stream framework lets you easily build highly scalable event-driven microservices connected with shared messaging systems. Furthermore there is an extensive set (60+) of pre-built streaming applications that one can use out of the box. The Streaming-Runtime Processor resource provides seamless support for deploying Spring Cloud Stream application. Lest combine the time-source , transformer and log-sink streaming applications into a simple streaming pipeline: The time source processor generates timestamps at fixed, pre-configured intervals and emits them to the timestamps stream . The time.date-format property sets the desired date format. The transformer processor uses SpEL expressions to convert the input message payload and send the result to the output uppercase stream. Here the expression converts the payload to uppercase. The log sink processor listens for input messages from the uppercase stream and prints them to the standard console output. The log.expression property allows us to specify the format of the printed log messages. The Streams exchange plain text payloads (e.g. byte-array) and do not require dedicated schema definitions. The pre-built SCS streaming applications are build to support same message broker as input and output stream. For each application there are two builds one with Apache Kafka and another with RabbitMQ . It is still possible to re-compile those applications for different message brokers (e.g. protocols ) or even mixture for multiple protocols (e.g. multibinder ). The streaming-pipeline-tiktock.yaml puts the entire pipeline together and after deployed would look like: The streaming-pipeline-ticktock-partitioned-better.yaml shows how to data-partition the TickTock application leveraging the SCS Data-Partitioning capabilities.","title":"Ticktock Pipeline"},{"location":"samples/spring-cloud-stream/tick-tock/#quick-start","text":"Follow the Streaming Runtime Install instructions to instal the operator. Install the anomaly detection streaming application: ticktock ticktock - partitioned kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/spring-cloud-stream/streaming-pipeline-ticktock.yaml' -n streaming-runtime kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/spring-cloud-stream/streaming-pipeline-ticktock-partitioned-better.yaml' -n streaming-runtime Follow the explore results instructions to see what data is generated and how it is processed though the pipeline. To delete the data pipeline and the data generator: kubectl delete srs,srcs,srp --all -n streaming-runtime","title":"Quick start"},{"location":"samples/top-k-songs/top-k-songs/","text":"Top-K Songs By Genre Music ranking application that continuously computes the top 3 most played songs by genre, based on song play events collected in real-time. ( inspired by the music sample. ) The application is modelled as a streaming music service with two input streams: songs and playevents and outputs stream-out . The Stream and Processor streaming runtime resources can help to model the music ranking applications. The desired data pipeline would look something like ths: The songs stream is a feed of the songs known to the streaming service. It provides detailed information for each song. When a new song is released, the recording company sends a new song-event to the songs stream. The playevents stream on the other hand is a feed of songs being played by streaming service. The song-join Processor enriches the playevents stream by joining it with the songs stream. The result songplays stream contains the streamed songs along with details such as song name and genre. The Processor uses streaming- SQL to implement the stream join operation: INSERT INTO [[ STREAM : songplays ]] SELECT Plays . song_id , Songs . album , Songs . artist , Songs . name , Songs . genre , Plays . duration , Plays . event_time FROM ( SELECT * FROM [[ STREAM : playevents ]] WHERE duration >= 30000 ) AS Plays INNER JOIN [[ STREAM : songs ]] AS Songs ON Plays . song_id = Songs . song_id We, also, filter the play events to only accept events where the duration is > 30 seconds. Next, the song-aggregate Processor groups the songplays stream by name and genre over a time-windowed interval and continuously compute the top 3 songs per genre over this interval. This effectively computes the top-k aggregate and when expressed in streaming SQL would look like this: INSERT INTO [[ STREAM : topk - songs - per - genre ]] SELECT window_start , window_end , song_id , name , genre , play_count FROM ( SELECT * , ROW_NUMBER () OVER ( PARTITION BY window_start , window_end , genre ORDER BY play_count DESC ) AS row_num FROM ( SELECT window_start , window_end , song_id , name , genre , COUNT ( * ) AS play_count FROM TABLE ( TUMBLE ( TABLE [[ STREAM : songplays ]], DESCRIPTOR ( event_time ), INTERVAL '60' SECONDS )) GROUP BY window_start , window_end , song_id , name , genre ) ) WHERE row_num <= 3 The aggregated topk-songs-per-genre stream emits every minute the top 3 songs per genre. Next the song-udf Processor is configured with a User Defined Function ( UDF ) to alter the payload programmatically send the result downstream to the stream-out stream. For this demo the song-udf Processor is configured with simple Python UDF that simply converts the input payload to uppercase: class MessageService ( MessageService_pb2_grpc . MessagingServiceServicer ): def requestReply ( self , request , context ): print ( \"Server received Payload: %s and Headers: %s \" % ( request . payload . decode (), request . headers )) return MessageService_pb2 . GrpcMessage ( payload = str . encode ( request . payload . decode () . upper ()), headers = request . headers ) The UDF can be written in any programming language as long as it adhere to the User Defined Function contract. The streaming-pipeline.yaml uses the Stream and Processor resources to implement and deploy the music chart application: Quick start Follow the Streaming Runtime Install instructions to instal the operator. Deploy the Music Chart streaming pipeline. Three alternative deployment configurations are provided to demonstrate different approaches to define the payload schemas. streaming-pipeline.yaml with SQL schema with Avro schema with Avro Schema Registry kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/top-k-songs/streaming-pipeline.yaml' -n streaming-runtime kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/top-k-songs/streaming-pipeline-inline-sql-schema.yaml' -n streaming-runtime kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/top-k-songs/streaming-pipeline-inline-avro-schema.yaml' -n streaming-runtime kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/top-k-songs/streaming-pipeline-inline-avro-confluent-schema.yaml' -n streaming-runtime Note: you can choose between different Stream schema definitions approaches, selecting between sr-native, avro, sql-ddl and use remote schema registry. Start the input message generator. Messages are encoded in Avro, using the same schemas defined by the songs and playevents Streams and send to the topics defined in those streams. kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/top-k-songs/data-generator.yaml' -n streaming-runtime Follow the instructions to explore the results . Use the kubectl get srcs,srs,srp -n streaming-runtime to list all Streaming Runtime resources: kubectl get srcs,srs,srp -n streaming-runtime NAME READY REASON clusterstream.streaming.tanzu.vmware.com/cluster-stream-kafka-playevents true ProtocolDeployed clusterstream.streaming.tanzu.vmware.com/cluster-stream-kafka-songplays true ProtocolDeployed clusterstream.streaming.tanzu.vmware.com/cluster-stream-kafka-songs true ProtocolDeployed clusterstream.streaming.tanzu.vmware.com/cluster-stream-kafka-topk-songs-per-genre true ProtocolDeployed clusterstream.streaming.tanzu.vmware.com/cluster-stream-rabbitmq-out true ProtocolDeployed NAME READY REASON stream.streaming.tanzu.vmware.com/kafka-stream-playevents true StreamDeployed stream.streaming.tanzu.vmware.com/kafka-stream-songplays true StreamDeployed stream.streaming.tanzu.vmware.com/kafka-stream-songs true StreamDeployed stream.streaming.tanzu.vmware.com/kafka-stream-topk-songs-per-genre true StreamDeployed stream.streaming.tanzu.vmware.com/rabbitmq-stream-out true StreamDeployed NAME READY REASON processor.streaming.tanzu.vmware.com/topk-songs-aggregate true ProcessorDeployed processor.streaming.tanzu.vmware.com/topk-songs-join true ProcessorDeployed processor.streaming.tanzu.vmware.com/topk-songs-udf true ProcessorDeployed Delete the music chart pipeline and the data generator: kubectl delete srs,srcs,srp --all -n streaming-runtime kubectl delete deployments,svc -l app = top-k-songs-data-generator -n streaming-runtime","title":"Music Chart"},{"location":"samples/top-k-songs/top-k-songs/#top-k-songs-by-genre","text":"Music ranking application that continuously computes the top 3 most played songs by genre, based on song play events collected in real-time. ( inspired by the music sample. ) The application is modelled as a streaming music service with two input streams: songs and playevents and outputs stream-out . The Stream and Processor streaming runtime resources can help to model the music ranking applications. The desired data pipeline would look something like ths: The songs stream is a feed of the songs known to the streaming service. It provides detailed information for each song. When a new song is released, the recording company sends a new song-event to the songs stream. The playevents stream on the other hand is a feed of songs being played by streaming service. The song-join Processor enriches the playevents stream by joining it with the songs stream. The result songplays stream contains the streamed songs along with details such as song name and genre. The Processor uses streaming- SQL to implement the stream join operation: INSERT INTO [[ STREAM : songplays ]] SELECT Plays . song_id , Songs . album , Songs . artist , Songs . name , Songs . genre , Plays . duration , Plays . event_time FROM ( SELECT * FROM [[ STREAM : playevents ]] WHERE duration >= 30000 ) AS Plays INNER JOIN [[ STREAM : songs ]] AS Songs ON Plays . song_id = Songs . song_id We, also, filter the play events to only accept events where the duration is > 30 seconds. Next, the song-aggregate Processor groups the songplays stream by name and genre over a time-windowed interval and continuously compute the top 3 songs per genre over this interval. This effectively computes the top-k aggregate and when expressed in streaming SQL would look like this: INSERT INTO [[ STREAM : topk - songs - per - genre ]] SELECT window_start , window_end , song_id , name , genre , play_count FROM ( SELECT * , ROW_NUMBER () OVER ( PARTITION BY window_start , window_end , genre ORDER BY play_count DESC ) AS row_num FROM ( SELECT window_start , window_end , song_id , name , genre , COUNT ( * ) AS play_count FROM TABLE ( TUMBLE ( TABLE [[ STREAM : songplays ]], DESCRIPTOR ( event_time ), INTERVAL '60' SECONDS )) GROUP BY window_start , window_end , song_id , name , genre ) ) WHERE row_num <= 3 The aggregated topk-songs-per-genre stream emits every minute the top 3 songs per genre. Next the song-udf Processor is configured with a User Defined Function ( UDF ) to alter the payload programmatically send the result downstream to the stream-out stream. For this demo the song-udf Processor is configured with simple Python UDF that simply converts the input payload to uppercase: class MessageService ( MessageService_pb2_grpc . MessagingServiceServicer ): def requestReply ( self , request , context ): print ( \"Server received Payload: %s and Headers: %s \" % ( request . payload . decode (), request . headers )) return MessageService_pb2 . GrpcMessage ( payload = str . encode ( request . payload . decode () . upper ()), headers = request . headers ) The UDF can be written in any programming language as long as it adhere to the User Defined Function contract. The streaming-pipeline.yaml uses the Stream and Processor resources to implement and deploy the music chart application:","title":"Top-K Songs By Genre"},{"location":"samples/top-k-songs/top-k-songs/#quick-start","text":"Follow the Streaming Runtime Install instructions to instal the operator. Deploy the Music Chart streaming pipeline. Three alternative deployment configurations are provided to demonstrate different approaches to define the payload schemas. streaming-pipeline.yaml with SQL schema with Avro schema with Avro Schema Registry kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/top-k-songs/streaming-pipeline.yaml' -n streaming-runtime kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/top-k-songs/streaming-pipeline-inline-sql-schema.yaml' -n streaming-runtime kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/top-k-songs/streaming-pipeline-inline-avro-schema.yaml' -n streaming-runtime kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/top-k-songs/streaming-pipeline-inline-avro-confluent-schema.yaml' -n streaming-runtime Note: you can choose between different Stream schema definitions approaches, selecting between sr-native, avro, sql-ddl and use remote schema registry. Start the input message generator. Messages are encoded in Avro, using the same schemas defined by the songs and playevents Streams and send to the topics defined in those streams. kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/top-k-songs/data-generator.yaml' -n streaming-runtime Follow the instructions to explore the results . Use the kubectl get srcs,srs,srp -n streaming-runtime to list all Streaming Runtime resources: kubectl get srcs,srs,srp -n streaming-runtime NAME READY REASON clusterstream.streaming.tanzu.vmware.com/cluster-stream-kafka-playevents true ProtocolDeployed clusterstream.streaming.tanzu.vmware.com/cluster-stream-kafka-songplays true ProtocolDeployed clusterstream.streaming.tanzu.vmware.com/cluster-stream-kafka-songs true ProtocolDeployed clusterstream.streaming.tanzu.vmware.com/cluster-stream-kafka-topk-songs-per-genre true ProtocolDeployed clusterstream.streaming.tanzu.vmware.com/cluster-stream-rabbitmq-out true ProtocolDeployed NAME READY REASON stream.streaming.tanzu.vmware.com/kafka-stream-playevents true StreamDeployed stream.streaming.tanzu.vmware.com/kafka-stream-songplays true StreamDeployed stream.streaming.tanzu.vmware.com/kafka-stream-songs true StreamDeployed stream.streaming.tanzu.vmware.com/kafka-stream-topk-songs-per-genre true StreamDeployed stream.streaming.tanzu.vmware.com/rabbitmq-stream-out true StreamDeployed NAME READY REASON processor.streaming.tanzu.vmware.com/topk-songs-aggregate true ProcessorDeployed processor.streaming.tanzu.vmware.com/topk-songs-join true ProcessorDeployed processor.streaming.tanzu.vmware.com/topk-songs-udf true ProcessorDeployed Delete the music chart pipeline and the data generator: kubectl delete srs,srcs,srp --all -n streaming-runtime kubectl delete deployments,svc -l app = top-k-songs-data-generator -n streaming-runtime","title":"Quick start"},{"location":"why/why-streaming-runtime/","text":"Why Streaming Runtime? Control Plane vs Data Plane As a design pattern, the Control Plane ( CP ) is a sub-system that defines and controls how the work should be done. The Data Plane ( DP ) on the other hand is where the actual work is done. The separation of concerns allows innovating and scaling the Data Plane and Control Plane independently. In the context of the Streaming Runtime the Data Plane is where most of the data transformations happen and it is optimized for speed of processing, availability , simplicity and regularity . The Control Plane controls the Data Plane and is optimized for decision making and in general facilitating and simplifying the Data Plane processing. The Control Plane instantiates and tears down Processors as needed, provisions and configures the infrastructure backing the Streams and ClusterStream messaging middleware. The Control Plane manages the partitioning, scaling and internal states of the pipeline run in the Data Plane. Kubernetes itself is designed around the Control Plane and Data Plane principles. It is comprised of independent and composable process controllers that continuously drive the current state toward the provided desired state . Controllers operate on a collection of API objects of a certain kind; for example, the built-in pods resource contains a collection of Pod objects. Kubernetes is extensible, allowing to add new custom APIs and process controllers. This provides us with a framework to build and run distributed applications resiliently! The framework provides the building blocks for building developer platforms, but preserves user choice and flexibility where it is important. Kubernetes takes care of scaling and failover and self-healing for your custom platforms, and provides deployment patterns, such as canary or blue/green deployments. Among others it provides: Components using the apiserver benefit from common access control, audit logging, and policy extension The Kubernetes apiserver maintains its own strongly-consistent storage via etcd, reducing the number of storage backends needed. common tools for managing API problems, such as validation and version changes. Lastly, using the apiserver to host additional resources allows the resources to be managed with the same tooling as built-in resources. Another advantage of using Kubernetes as a Framework to build the Streaming Runtime is the the existing rich ecosystem of Kubernetes operators ( https://operatorhub.io ) : For example the RabbitMQ Operator or Strimzi Operator all provisioning and managing RabbitMQ or Kafka cluster in Kubernetes environments. Steaming vs Batch Processing In Batch processing the processing and analysis happens on a set of data that have already been stored over a period of time. An example is payroll and billing systems that have to be processed weekly or monthly. While the Table/Batch processing operates on data-at-rest ( e.g. bounded datasets), the streaming data processing operates on data-at-motion (unbounded datasets). The stream processing is defined as the processing of an unbounded amount of data without interaction or interruption. Business cases for stream processing include: real-time credit card fraud detection or predictive analytics or near-real-time business data processing for actionable analytics.","title":"Why SR"},{"location":"why/why-streaming-runtime/#why-streaming-runtime","text":"","title":"Why Streaming Runtime?"},{"location":"why/why-streaming-runtime/#control-plane-vs-data-plane","text":"As a design pattern, the Control Plane ( CP ) is a sub-system that defines and controls how the work should be done. The Data Plane ( DP ) on the other hand is where the actual work is done. The separation of concerns allows innovating and scaling the Data Plane and Control Plane independently. In the context of the Streaming Runtime the Data Plane is where most of the data transformations happen and it is optimized for speed of processing, availability , simplicity and regularity . The Control Plane controls the Data Plane and is optimized for decision making and in general facilitating and simplifying the Data Plane processing. The Control Plane instantiates and tears down Processors as needed, provisions and configures the infrastructure backing the Streams and ClusterStream messaging middleware. The Control Plane manages the partitioning, scaling and internal states of the pipeline run in the Data Plane. Kubernetes itself is designed around the Control Plane and Data Plane principles. It is comprised of independent and composable process controllers that continuously drive the current state toward the provided desired state . Controllers operate on a collection of API objects of a certain kind; for example, the built-in pods resource contains a collection of Pod objects. Kubernetes is extensible, allowing to add new custom APIs and process controllers. This provides us with a framework to build and run distributed applications resiliently! The framework provides the building blocks for building developer platforms, but preserves user choice and flexibility where it is important. Kubernetes takes care of scaling and failover and self-healing for your custom platforms, and provides deployment patterns, such as canary or blue/green deployments. Among others it provides: Components using the apiserver benefit from common access control, audit logging, and policy extension The Kubernetes apiserver maintains its own strongly-consistent storage via etcd, reducing the number of storage backends needed. common tools for managing API problems, such as validation and version changes. Lastly, using the apiserver to host additional resources allows the resources to be managed with the same tooling as built-in resources. Another advantage of using Kubernetes as a Framework to build the Streaming Runtime is the the existing rich ecosystem of Kubernetes operators ( https://operatorhub.io ) : For example the RabbitMQ Operator or Strimzi Operator all provisioning and managing RabbitMQ or Kafka cluster in Kubernetes environments.","title":"Control Plane vs Data Plane"},{"location":"why/why-streaming-runtime/#steaming-vs-batch-processing","text":"In Batch processing the processing and analysis happens on a set of data that have already been stored over a period of time. An example is payroll and billing systems that have to be processed weekly or monthly. While the Table/Batch processing operates on data-at-rest ( e.g. bounded datasets), the streaming data processing operates on data-at-motion (unbounded datasets). The stream processing is defined as the processing of an unbounded amount of data without interaction or interruption. Business cases for stream processing include: real-time credit card fraud detection or predictive analytics or near-real-time business data processing for actionable analytics.","title":"Steaming vs Batch Processing"}]}